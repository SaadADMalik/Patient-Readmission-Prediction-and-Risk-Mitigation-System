{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":11774159,"sourceType":"datasetVersion","datasetId":7392178},{"sourceId":11774937,"sourceType":"datasetVersion","datasetId":7392670},{"sourceId":11775100,"sourceType":"datasetVersion","datasetId":7392769},{"sourceId":11784582,"sourceType":"datasetVersion","datasetId":7398996},{"sourceId":11847080,"sourceType":"datasetVersion","datasetId":7443750},{"sourceId":400945,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":328080,"modelId":348933}],"dockerImageVersionId":31012,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!java -version\n\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\nimport subprocess\nimport os\nimport shutil\nfrom datetime import datetime\n\n# Clear existing files to free space\nprint(\"Clearing existing files...\")\nif os.path.exists('healthcare_data.csv.gz'):\n    os.remove('healthcare_data.csv.gz')\nif os.path.exists('output'):\n    shutil.rmtree('output')\n!rm -rf *\n!du -h --max-depth=1 .\nprint(\"Storage check before starting:\")\n!df -h\n\n# Download Synthea if not already downloaded\nif not os.path.exists('synthea.jar'):\n    print(\"Downloading Synthea JAR...\")\n    subprocess.run(\"wget -O synthea.jar https://github.com/synthetichealth/synthea/releases/download/v3.2.0/synthea-with-dependencies.jar\", shell=True)\nprint(\"Synthea JAR ready.\")\n\n# Initialize data\ntarget_rows = 1000000\ndf = pd.DataFrame()\n\n# Generate 2,500 patients per iteration until ~1M rows\nprint(\"Starting data generation...\")\nfor i in range(80):\n    print(f\"Processing Batch {i+1}...\")\n    print(\"Storage check before batch:\")\n    !df -h\n    if os.path.exists('output'):\n        shutil.rmtree('output')\n    subprocess.run(\"java -Xmx2g -jar synthea.jar -p 2500 --exporter.csv.export true\", shell=True)\n    patients = pd.read_csv('output/csv/patients.csv')\n    encounters = pd.read_csv('output/csv/encounters.csv')\n    # Corrected merge using 'PATIENT' and 'Id'\n    batch_df = encounters.merge(patients, left_on='PATIENT', right_on='Id', how='left')\n    batch_df['START'] = pd.to_datetime(batch_df['START'])\n    # Calculate AGE from BIRTHDATE if not present\n    if 'AGE' not in batch_df.columns:\n        # Convert BIRTHDATE to datetime, calculate difference in days, then convert to years\n        birth_dates = pd.to_datetime(batch_df['BIRTHDATE'], errors='coerce')\n        time_diff = (datetime.now() - birth_dates).dt.total_seconds() / (365.25 * 24 * 60 * 60)\n        batch_df['AGE'] = time_diff.astype(float).round(0).astype(int)  # Round to nearest integer\n    # Drop rows where REASONCODE or AGE is NaN\n    batch_df.dropna(subset=['REASONCODE', 'AGE'], inplace=True)\n    if len(batch_df) > 0:\n        df = pd.concat([df, batch_df], ignore_index=True)\n        df = df.head(target_rows)\n        df.to_csv('healthcare_data.csv.gz', compression='gzip', index=False)\n        print(f\"Batch {i+1} completed. Total rows: {len(df)}\")\n        if len(df) >= target_rows:\n            break\n    if os.path.exists('output'):\n        shutil.rmtree('output')\n    print(\"Storage check after batch:\")\n    !df -h\n\n# Final save\noutput_file = 'healthcare_data.csv.gz'\ndf.to_csv(output_file, compression='gzip', index=False)\nprint(f\"Final dataset size: {len(df)} rows\")\nprint(\"Final file size check:\")\n!du -h healthcare_data.csv.gz","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!ls /kaggle/input/","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!ls /kaggle/input/healthcare-data-csv/healthcare_data.csv","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Load the dataset from the Kaggle input directory\nfile_path = '/kaggle/input/healthcare-data-csv/healthcare_data.csv'\ndf = pd.read_csv(file_path)  # No compression since it's .csv, not .gz\nprint(f\"Loaded dataset with {len(df)} rows and {len(df.columns)} columns\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"pd.set_option('display.max_columns', None)\ndf.head()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# PREPROCESSING\n","metadata":{}},{"cell_type":"code","source":"df.sort_values([\"PATIENT\", \"START\"])\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df.info","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Remove duplicates\ninitial_rows = len(df)\ndf = df.drop_duplicates(subset=['PATIENT', 'START'], keep='first')\nduplicates_removed = initial_rows - len(df)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df.info()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df.isnull().sum()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# dropping null values\ndf = df.dropna(subset=['STOP', 'START'])\nprint(f\"Dropped {initial_rows - len(df)} rows with missing STOP or START\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df.tail()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# These arent useful columns\ndf['SSN'] = df['SSN'].fillna('Unknown')\ndf['DEATHDATE'] = df['DEATHDATE'].fillna('Unknown')\ndf['DRIVERS'] = df['DRIVERS'].fillna('Unknown')\ndf['PASSPORT'] = df['PASSPORT'].fillna('Unknown')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# CHeckingAGE is reasonable\ndf = df[df['AGE'] > 0]\n\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"datetime_columns = ['START', 'STOP', 'BIRTHDATE', 'DEATHDATE'] \nfor col in datetime_columns:\n    if col in df.columns:\n        df[col] = pd.to_datetime(df[col], errors='coerce')\n        print(f\"{col} column type after conversion: {df[col].dtype}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df.info()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Calculate encounter duration (in hours)\ndf['DURATION_HOURS'] = (pd.to_datetime(df['STOP']) - pd.to_datetime(df['START'])).dt.total_seconds() / 3600","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df.drop('DURATION_HOURS', axis = 1, inplace = True)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df['PREV_ENCOUNTER'] = df.groupby('PATIENT')['START'].shift(1)\ndf['DAYS_SINCE_LAST'] = (df['START'] - df['PREV_ENCOUNTER']).dt.total_seconds() / (24 * 3600)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"unique_patients = df['PATIENT'].nunique()\nprint(unique_patients)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Flag readmissions\ndf['READMISSION_30D'] = df['DAYS_SINCE_LAST'].apply(lambda x: 1 if pd.notnull(x) and x <= 30 else 0)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Encounter frequency per patient\ndf['ENCOUNTER_FREQ'] = df.groupby('PATIENT')['START'].transform('count')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df.head()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#deceased status\ndf['IS_DECEASED'] = (df['DEATHDATE'] != 'Unknown').astype(int)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"cost_cap = df['TOTAL_CLAIM_COST'].quantile(0.99)\ndf['TOTAL_CLAIM_COST'] = df['TOTAL_CLAIM_COST'].clip(upper=cost_cap)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#changing male to 0 and female to 1\ndf['GENDER'] = df['GENDER'].map({'M': 0, 'F': 1})\ndf['ENCOUNTERCLASS'] = df['ENCOUNTERCLASS'].astype('category').cat.codes","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# EDA\n","metadata":{}},{"cell_type":"code","source":"# Age distribution\nplt.figure(figsize=(10, 6))\nsns.histplot(df['AGE'], bins=30, kde=True)\nplt.title('Age Distribution of Patients')\nplt.xlabel('Age')\nplt.ylabel('Frequency')\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Readmission rate by encounter class\nreadmission_by_class = df.groupby('ENCOUNTERCLASS')['READMISSION_30D'].mean()\nplt.figure(figsize=(10, 6))\nreadmission_by_class.plot(kind='bar')\nplt.title('Readmission rate by encounter class (within 30 days)')\nplt.xlabel('Encounter Class (Encoded)')\nplt.ylabel('Readmission Rate')\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Corr\nnumeric_cols = ['AGE', 'DURATION_HOURS', 'DAYS_SINCE_LAST', 'ENCOUNTER_FREQ', 'READMISSION_30D', 'BASE_ENCOUNTER_COST', 'TOTAL_CLAIM_COST', 'PAYER_COVERAGE', 'IS_DECEASED']\ncorr = df[numeric_cols].corr()\nplt.figure(figsize=(8, 6))\nsns.heatmap(corr, annot=True, cmap='coolwarm', vmin=-1, vmax=1)\nplt.title('Correlation Matrix of Numeric Features')\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df.to_csv('/kaggle/working/healthcare_data_engineered.csv.gz', compression='gzip', index=False)\nprint(\"Feature engineering and EDA completed. Engineered dataset saved as '/kaggle/working/healthcare_data_engineered.csv.gz'\")\nprint(\"File size check:\")\n!du -h /kaggle/working/healthcare_data_engineered.csv.gz","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# TIME SERIES ANALYSIS\n","metadata":{}},{"cell_type":"code","source":"import pandas as pd\ndf = pd.read_csv('/kaggle/input/healthcare-data-engineered-csv-gz/healthcare_data_engineered.csv' )\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df.columns","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df.head()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df.info()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df['START'] = pd.to_datetime(df['START'], errors='coerce')\ndf['YEAR'] = df['START'].dt.year\ndf['MONTH'] = df['START'].dt.month\ndf['DAY'] = df['START'].dt.day","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df.head()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Readmission rate by month\nmonthly_readmissions = df.groupby(['YEAR', 'MONTH'])['READMISSION_30D'].mean().reset_index()\nmonthly_readmissions['DATE'] = pd.to_datetime(monthly_readmissions[['YEAR', 'MONTH']].assign(DAY=1))\nprint(monthly_readmissions.head())","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"plt.figure(figsize=(10, 5))\nplt.plot(monthly_readmissions['DATE'], monthly_readmissions['READMISSION_30D'], label='Readmission Rate')\nplt.title('Monthly Readmission Rate Over Time')\nplt.xlabel('Date')\nplt.ylabel('Readmission Rate (within 30 days)')\nplt.xticks(rotation=45)\nplt.legend()\nplt.show()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#Encounter Frequency by Month\nmonthly_encounters = df.groupby(['YEAR', 'MONTH'])['ENCOUNTER_FREQ'].mean().reset_index()\nmonthly_encounters['DATE'] = pd.to_datetime(monthly_encounters[['YEAR', 'MONTH']].assign(DAY=1))\nprint(monthly_encounters.head())","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"plt.figure(figsize=(10, 5))\nplt.plot(monthly_encounters['DATE'], monthly_encounters['ENCOUNTER_FREQ'], label='Average Encounters per Patient', color='orange')\nplt.title('Average Encounter Frequency Over Time')\nplt.xlabel('Date')\nplt.ylabel('Average Encounters')\nplt.xticks(rotation=45)\nplt.legend()\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#Claim Cost by Month\nmonthly_costs = df.groupby(['YEAR', 'MONTH'])['TOTAL_CLAIM_COST'].mean().reset_index()\nmonthly_costs['DATE'] = pd.to_datetime(monthly_costs[['YEAR', 'MONTH']].assign(DAY=1))\nprint(monthly_costs.head())","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"plt.figure(figsize=(10, 5))\nplt.plot(monthly_costs['DATE'], monthly_costs['TOTAL_CLAIM_COST'], label='Average Total Claim Cost', color='green')\nplt.title('Average Total Claim Cost Over Time')\nplt.xlabel('Date')\nplt.ylabel('Average Cost ($)')\nplt.xticks(rotation=45)\nplt.legend()\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"plt.figure(figsize=(12, 8))\n\nplt.subplot(3, 1, 1)\nplt.plot(monthly_readmissions['DATE'], monthly_readmissions['READMISSION_30D'], label='Readmission Rate')\nplt.title('Monthly Readmission Rate Over Time')\nplt.xlabel('Date')\nplt.ylabel('Readmission Rate (within 30 days)')\nplt.xticks(rotation=45)\nplt.legend()\n\nplt.subplot(3, 1, 2)\nplt.plot(monthly_encounters['DATE'], monthly_encounters['ENCOUNTER_FREQ'], label='Average Encounters per Patient', color='orange')\nplt.title('Average Encounter Frequency Over Time')\nplt.xlabel('Date')\nplt.ylabel('Average Encounters')\nplt.xticks(rotation=45)\nplt.legend()\n\nplt.subplot(3, 1, 3)\nplt.plot(monthly_costs['DATE'], monthly_costs['TOTAL_CLAIM_COST'], label='Average Total Claim Cost', color='green')\nplt.title('Average Total Claim Cost Over Time')\nplt.xlabel('Date')\nplt.ylabel('Average Cost ($)')\nplt.xticks(rotation=45)\nplt.legend()\n\nplt.tight_layout()\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Step 3.5: Analyze trends by age group\ndf['AGE_GROUP'] = pd.cut(df['AGE'], bins=[0, 20, 40, 60, 80, 100], labels=['0-20', '21-40', '41-60', '61-80', '81+'])\nage_group_readmissions = df.groupby(['YEAR', 'MONTH', 'AGE_GROUP'])['READMISSION_30D'].mean().reset_index()\nage_group_readmissions['DATE'] = pd.to_datetime(age_group_readmissions[['YEAR', 'MONTH']].assign(DAY=1))\nprint(age_group_readmissions)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"plt.figure(figsize=(12, 6))\nfor age_group in age_group_readmissions['AGE_GROUP'].unique():\n    subset = age_group_readmissions[age_group_readmissions['AGE_GROUP'] == age_group]\n    plt.plot(subset['DATE'], subset['READMISSION_30D'], label=age_group)\nplt.title('Readmission Rate by Age Group Over Time')\nplt.xlabel('Date')\nplt.ylabel('Readmission Rate (within 30 days)')\nplt.xticks(rotation=45)\nplt.legend()\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df.to_csv('/kaggle/working/healthcare_data_timeseries.csv.gz', compression='gzip', index=False)\nprint(\"Time-series analysis completed. Dataset saved as '/kaggle/working/healthcare_data_timeseries.csv.gz'\")\nprint(\"File size check:\")\n!du -h /kaggle/working/healthcare_data_timeseries.csv.gz","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(df[['YEAR', 'MONTH', 'READMISSION_30D', 'TOTAL_CLAIM_COST', 'ENCOUNTER_FREQ', 'AGE_GROUP']].describe())","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"import pandas as pd\ndatetime_cols = ['START', 'STOP', 'BIRTHDATE', 'DEATHDATE', 'PREV_ENCOUNTER']\n                 \ndf = pd.read_csv('/kaggle/input/healthcare-data-timeseries-csv-gz/healthcare_data_timeseries.csv', parse_dates=datetime_cols)\n\nprint(df[['YEAR', 'MONTH', 'READMISSION_30D', 'TOTAL_CLAIM_COST', 'ENCOUNTER_FREQ', 'AGE_GROUP']].describe())","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"for col in datetime_cols:\n    if col in df.columns:\n        df[col] = pd.to_datetime(df[col], errors='coerce')\n        print(f\"{col} column type after conversion: {df[col].dtype}\")\n\ndf['YEAR'] = df['START'].dt.year\ndf = df[(df['YEAR'] >= 2016) & (df['YEAR'] <= 2025)]\nprint(\"Total rows after YEAR filter:\", len(df))\nprint(\"Unique patients after YEAR filter:\", df['PATIENT'].nunique())","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(df[['START', 'PREV_ENCOUNTER', 'DAYS_SINCE_LAST', 'READMISSION_30D']].isna().sum())","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(\"ENCOUNTERCLASS distribution:\")\nprint(df['ENCOUNTERCLASS'].value_counts())","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(\"\\nDAYS_SINCE_LAST distribution:\")\nprint(df['DAYS_SINCE_LAST'].describe())\nprint(\"Rows with DAYS_SINCE_LAST <= 30:\")\nprint(len(df[df['DAYS_SINCE_LAST'] <= 30]))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(\"\\nNumber of unique patients:\", df['PATIENT'].nunique())\nprint(\"Average encounters per patient:\", df.groupby('PATIENT')['START'].count().mean())","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df.head()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Check unique ENCOUNTERCLASS values and their corresponding REASONDESCRIPTION\nprint(df.groupby('ENCOUNTERCLASS')['REASONDESCRIPTION'].unique())","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"inpatient_codes = [1, 2, 6, 7, 9]\ndf['IS_INPATIENT'] = df['ENCOUNTERCLASS'].isin(inpatient_codes).astype(int)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df.info()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df = df.sort_values(['PATIENT', 'START'])\ndf['PREV_ENCOUNTER'] = df.groupby('PATIENT')['START'].shift(1)\ndf['PREV_ENCOUNTER'] = pd.to_datetime(df['PREV_ENCOUNTER'], errors='coerce')\ndf['DAYS_SINCE_LAST'] = (df['START'] - df['PREV_ENCOUNTER']).dt.days\ndf['READMISSION_30D'] = ((df['IS_INPATIENT'] == 1) & \n                         (df['DAYS_SINCE_LAST'].notnull()) & \n                         (df['DAYS_SINCE_LAST'].between(1, 60))).astype(int)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df['ENCOUNTER_FREQ'] = df.groupby('PATIENT')['START'].transform('count')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(\"\\nSample ENCOUNTER_FREQ values:\")\nprint(df[['PATIENT', 'ENCOUNTER_FREQ']].head(10))  # Check a sample\nprint(\"ENCOUNTER_FREQ mean (calculated):\", df['ENCOUNTER_FREQ'].mean())\nprint(\"Expected ENCOUNTER_FREQ mean (total rows / unique patients):\", len(df) / df['PATIENT'].nunique())","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(\"Updated READMISSION_30D mean:\", df['READMISSION_30D'].mean())\nprint(\"Rows with READMISSION_30D = 1:\", len(df[df['READMISSION_30D'] == 1]))\nprint(\"ENCOUNTER_FREQ mean:\", df['ENCOUNTER_FREQ'].mean())\nprint(\"Total rows:\", len(df))\nprint(\"Unique patients:\", df['PATIENT'].nunique())","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(\"Duplicate rows:\", df.duplicated().sum())\nprint(\"Duplicate PATIENT IDs:\", df['PATIENT'].duplicated().sum())","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df['ENCOUNTER_FREQ'] = df.groupby('PATIENT')['START'].transform('count')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(\"Updated READMISSION_30D mean:\", df['READMISSION_30D'].mean())\nprint(\"Rows with READMISSION_30D = 1:\", len(df[df['READMISSION_30D'] == 1]))\nprint(\"ENCOUNTER_FREQ mean:\", df['ENCOUNTER_FREQ'].mean())\nprint(\"Total rows:\", len(df))\nprint(\"Unique patients:\", df['PATIENT'].nunique())","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df['ENCOUNTER_FREQ'].unique().sum()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df.to_csv('/kaggle/working/healthcare_data_final.csv.gz', compression='gzip', index=False)\nprint(\"Dataset saved as healthcare_data_final.csv.gz\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# MODEL TRAINING\n","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import train_test_split, GridSearchCV\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.preprocessing import OneHotEncoder, StandardScaler\nfrom sklearn.pipeline import Pipeline\nfrom xgboost import XGBClassifier\nfrom sklearn.metrics import classification_report, roc_auc_score, precision_recall_curve, auc\nimport matplotlib.pyplot as plt\n\ndata = pd.read_csv('/kaggle/input/healthcare-data-final-csv-gz/healthcare_data_final.csv')\nfor col in data.select_dtypes(include=['float64']).columns:\n    data[col] = data[col].astype('float32')\nfor col in data.select_dtypes(include=['int64']).columns:\n    data[col] = data[col].astype('int32')\n\nprint(f\"Dataset loaded with {data.shape[0]} rows and {data.shape[1]} columns.\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# 2. Drop high-cardinality columns, IS_INPATIENT, ENCOUNTER_FREQ, and DAYS_SINCE_LAST\ncolumns_to_drop = [\n    'Id_x', 'Id_y', 'PATIENT', 'ORGANIZATION', 'PROVIDER', 'PAYER',\n    'START', 'STOP', 'PREV_ENCOUNTER', 'BIRTHDATE', 'DEATHDATE',\n    'SSN', 'DRIVERS', 'PASSPORT', 'PREFIX', 'FIRST', 'LAST', 'SUFFIX', 'MAIDEN',\n    'ADDRESS', 'CITY', 'STATE', 'COUNTY', 'FIPS', 'ZIP',\n    'BIRTHPLACE', 'DESCRIPTION', 'REASONDESCRIPTION',\n    'IS_INPATIENT', 'ENCOUNTER_FREQ', 'DAYS_SINCE_LAST', 'CODE', 'ENCOUNTER_FREQ_NEW'  # Explicitly include CODE\n]\ndata = data.drop(columns=columns_to_drop, errors='ignore')\nprint(f\"Dropped columns. Remaining columns: {data.columns.tolist()}\")  # Debug print to confirm","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"X = data.drop('READMISSION_30D', axis=1)\ny = data['READMISSION_30D']","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\nprint(f\"Training set: {X_train.shape[0]} rows, Testing set: {X_test.shape[0]} rows\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"categorical_cols = X.select_dtypes(include=['object']).columns.tolist()\nnumerical_cols = X.select_dtypes(include=['float32', 'int32']).columns.tolist()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"for col in ['DAYS_SINCE_LAST', 'ENCOUNTER_FREQ', 'CODE', 'IS_INPATIENT']:\n    if col in X.columns:\n        print(f\"Warning: {col} still present in features. Dropping now.\")\n        X = X.drop(columns=col)\n        X_train = X_train.drop(columns=col)\n        X_test = X_test.drop(columns=col)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"for col in categorical_cols[:]:\n    if X[col].nunique() > 50:\n        print(f\"Dropping high-cardinality column from encoding: {col} ({X[col].nunique()} unique values)\")\n        categorical_cols.remove(col)\n        X = X.drop(columns=col)\n        X_train = X_train.drop(columns=col)\n        X_test = X_test.drop(columns=col)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"numerical_cols = X.select_dtypes(include=['float32', 'int32']).columns.tolist()\nprint(f\"Categorical columns: {categorical_cols}\")\nprint(f\"Numerical columns: {numerical_cols}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"preprocessor = ColumnTransformer(\n    transformers=[\n        ('num', StandardScaler(), numerical_cols),\n        ('cat', OneHotEncoder(handle_unknown='ignore', sparse_output=True), categorical_cols)\n    ])","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"scale_pos_weight = (len(y_train) - sum(y_train)) / sum(y_train)\nxgb = XGBClassifier(scale_pos_weight=scale_pos_weight, random_state=42, eval_metric='logloss')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"pipeline = Pipeline(steps=[('preprocessor', preprocessor), ('classifier', xgb)])\nprint(\"Training initial XGBoost model...\")\npipeline.fit(X_train, y_train)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"y_pred = pipeline.predict(X_test)\ny_pred_proba = pipeline.predict_proba(X_test)[:, 1]\nprint(\"Initial Classification Report:\\n\", classification_report(y_test, y_pred))\nprint(f\"AUC-ROC: {roc_auc_score(y_test, y_pred_proba):.4f}\")\nprecision, recall, _ = precision_recall_curve(y_test, y_pred_proba)\nauc_pr = auc(recall, precision)\nprint(f\"AUC-PR: {auc_pr:.4f}\")\nplt.figure(figsize=(8, 6))\nplt.plot(recall, precision, label=f'AUC-PR = {auc_pr:.4f}')\nplt.xlabel('Recall')\nplt.ylabel('Precision')\nplt.title('Precision-Recall Curve (Initial Model)')\nplt.legend()\nplt.grid(True)\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"param_grid = {\n    'classifier__learning_rate': [0.01, 0.1],\n    'classifier__max_depth': [3, 5],\n    'classifier__n_estimators': [100, 200]\n}\ngrid_search = GridSearchCV(pipeline, param_grid, scoring='recall', cv=3, n_jobs=-1)\nprint(\"Performing hyperparameter tuning...\")\ngrid_search.fit(X_train, y_train)\nbest_model = grid_search.best_estimator_\nprint(f\"Best parameters: {grid_search.best_params_}\")\nprint(f\"Best recall score from CV: {grid_search.best_score_:.4f}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"y_pred_best = best_model.predict(X_test)\ny_pred_proba_best = best_model.predict_proba(X_test)[:, 1]\nprint(\"Updated Classification Report (Tuned Model):\\n\", classification_report(y_test, y_pred_best))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.calibration import CalibratedClassifierCV\nprint(\"Calibrating probabilities...\")\ncalibrated_model = CalibratedClassifierCV(best_model, method='sigmoid', cv=3)\ncalibrated_model.fit(X_train, y_train)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"y_pred_calibrated = calibrated_model.predict(X_test)\ny_pred_proba_calibrated = calibrated_model.predict_proba(X_test)[:, 1]\nprint(\"Classification Report (Calibrated Model):\\n\", classification_report(y_test, y_pred_calibrated))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"thresholds = [0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.75, 0.8]\nfor thresh in thresholds:\n    y_pred_adjusted = (y_pred_proba_calibrated >= thresh).astype(int)\n    print(f\"\\nClassification Report (Calibrated, Threshold = {thresh}):\")\n    print(classification_report(y_test, y_pred_adjusted, zero_division=0))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"feature_names = numerical_cols + best_model.named_steps['preprocessor'].named_transformers_['cat'].get_feature_names_out(categorical_cols).tolist()\nimportances = best_model.named_steps['classifier'].feature_importances_\nmin_length = min(len(importances), len(feature_names))\nimportances = importances[:min_length]\nfeature_names = feature_names[:min_length]\nimportance_df = pd.DataFrame({'feature': feature_names, 'importance': importances})\nimportance_df = importance_df.sort_values(by='importance', ascending=False)\nprint(\"Top 10 Important Features:\\n\", importance_df.head(10))\nplt.figure(figsize=(10, 6))\nplt.barh(importance_df['feature'].head(10), importance_df['importance'].head(10))\nplt.xlabel('Importance')\nplt.title('Top 10 Feature Importances')\nplt.gca().invert_yaxis()\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.preprocessing import LabelEncoder\ndata_encoded = data.copy()\nle = LabelEncoder()\ndata_encoded['ENCOUNTERCLASS'] = le.fit_transform(data['ENCOUNTERCLASS'])\ncorr_matrix = data_encoded[['ENCOUNTERCLASS', 'DAYS_SINCE_LAST', 'READMISSION_30D']].corr()\nprint(\"Correlation Matrix:\\n\", corr_matrix)\nsns.heatmap(corr_matrix, annot=True, cmap='coolwarm', vmin=-1, vmax=1)\nplt.title('Correlation Matrix')\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Making the model better\n\n","metadata":{}},{"cell_type":"code","source":"# Uninstall all potentially conflicting packages\n!pip uninstall numpy scikit-learn imbalanced-learn pandas scipy matplotlib rich fsspec packaging toolz torch gensim pylibcugraph-cu12 rmm-cu12 nvidia-cublas-cu12 nvidia-cudnn-cu12 nvidia-cufft-cu12 nvidia-curand-cu12 nvidia-cusolver-cu12 nvidia-cusparse-cu12 nvidia-nvjitlink-cu12 -y -q","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Install only the required dependencies\n!pip install numpy==1.26.4 scikit-learn==1.4.2 imbalanced-learn==0.12.3 pandas==2.2.2 scipy==1.14.1 matplotlib==3.8.4 xgboost==2.0.3 shap==0.45.1 --force-reinstall -q","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"pip install scikit-learn==1.5.0 imbalanced-learn==0.12.3","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-18T12:17:14.967988Z","iopub.execute_input":"2025-05-18T12:17:14.968293Z","iopub.status.idle":"2025-05-18T12:17:26.774116Z","shell.execute_reply.started":"2025-05-18T12:17:14.968271Z","shell.execute_reply":"2025-05-18T12:17:26.773077Z"}},"outputs":[{"name":"stdout","text":"Collecting scikit-learn==1.5.0\n  Downloading scikit_learn-1.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (11 kB)\nCollecting imbalanced-learn==0.12.3\n  Downloading imbalanced_learn-0.12.3-py3-none-any.whl.metadata (8.3 kB)\nRequirement already satisfied: numpy>=1.19.5 in /usr/local/lib/python3.11/dist-packages (from scikit-learn==1.5.0) (1.26.4)\nRequirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn==1.5.0) (1.15.2)\nRequirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn==1.5.0) (1.4.2)\nRequirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn==1.5.0) (3.6.0)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.19.5->scikit-learn==1.5.0) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.19.5->scikit-learn==1.5.0) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.19.5->scikit-learn==1.5.0) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.19.5->scikit-learn==1.5.0) (2025.1.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.19.5->scikit-learn==1.5.0) (2022.1.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.19.5->scikit-learn==1.5.0) (2.4.1)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.19.5->scikit-learn==1.5.0) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.19.5->scikit-learn==1.5.0) (2022.1.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.19.5->scikit-learn==1.5.0) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.19.5->scikit-learn==1.5.0) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.19.5->scikit-learn==1.5.0) (2024.2.0)\nDownloading scikit_learn-1.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.3 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.3/13.3 MB\u001b[0m \u001b[31m71.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading imbalanced_learn-0.12.3-py3-none-any.whl (258 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m258.3/258.3 kB\u001b[0m \u001b[31m10.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hInstalling collected packages: scikit-learn, imbalanced-learn\n  Attempting uninstall: scikit-learn\n    Found existing installation: scikit-learn 1.2.2\n    Uninstalling scikit-learn-1.2.2:\n      Successfully uninstalled scikit-learn-1.2.2\n  Attempting uninstall: imbalanced-learn\n    Found existing installation: imbalanced-learn 0.13.0\n    Uninstalling imbalanced-learn-0.13.0:\n      Successfully uninstalled imbalanced-learn-0.13.0\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nbigframes 1.36.0 requires rich<14,>=12.4.4, but you have rich 14.0.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed imbalanced-learn-0.12.3 scikit-learn-1.5.0\nNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"import threadpoolctl\nthreadpoolctl.threadpool_limits(limits=1, user_api=\"openmp\")\n\n# Import necessary libraries\nimport pandas as pd\nimport numpy as np\nimport shap\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.preprocessing import StandardScaler, OneHotEncoder\nfrom sklearn.pipeline import Pipeline\nfrom xgboost import XGBClassifier\nfrom sklearn.calibration import CalibratedClassifierCV\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, brier_score_loss, precision_recall_curve\nfrom sklearn.impute import SimpleImputer\nfrom imblearn.over_sampling import SMOTE\nimport warnings\n\n# Suppress threadpoolctl and other warnings\nwarnings.filterwarnings(\"ignore\", category=UserWarning, module=\"threadpoolctl\")\nwarnings.filterwarnings(\"ignore\", category=RuntimeWarning)\n\n# Step 1: Load the dataset\nprint(\"Loading dataset...\")\ndata = pd.read_csv('/kaggle/input/healthcare-data-final-csv-gz/healthcare_data_final.csv', low_memory=False)\nprint(f\"Dataset loaded with {len(data)} rows and {len(data.columns)} columns.\")\n\n# Step 2: Calculate READMISSION_30D using START and STOP dates\nprint(\"Calculating READMISSION_30D based on encounter dates...\")\ndata['START'] = pd.to_datetime(data['START'], errors='coerce')\ndata['STOP'] = pd.to_datetime(data['STOP'], errors='coerce')\ndata = data.dropna(subset=['START', 'STOP'])\ndata = data.sort_values(['PATIENT', 'START'])\ndata['NEXT_ENCOUNTER'] = data.groupby('PATIENT')['START'].shift(-1)\ndata['DAYS_TO_NEXT'] = (data['NEXT_ENCOUNTER'] - data['STOP']).dt.days\ndata['READMISSION_30D'] = data['DAYS_TO_NEXT'].apply(lambda x: 1 if pd.notna(x) and x <= 30 else 0)\ndata = data.drop(columns=['NEXT_ENCOUNTER', 'DAYS_TO_NEXT'])\nprint(\"Note: READMISSION_30D includes all encounters within 30 days; future work to refine to inpatient-only readmissions recommended.\")\n\ncolumns_to_drop = [\n    'readmission_risk', 'high_risk_flag', 'REASONCODE', 'Id_x', 'Id_y', 'PATIENT',\n    'ORGANIZATION', 'PROVIDER', 'PAYER', 'START', 'STOP', 'PREV_ENCOUNTER', 'BIRTHDATE',\n    'DEATHDATE', 'SSN', 'DRIVERS', 'PASSPORT', 'PREFIX', 'FIRST', 'LAST', 'SUFFIX',\n    'MAIDEN', 'ADDRESS', 'CITY', 'STATE', 'COUNTY', 'FIPS', 'ZIP', 'BIRTHPLACE',\n    'DESCRIPTION', 'REASONDESCRIPTION', 'IS_INPATIENT', 'ENCOUNTER_FREQ',\n    'DAYS_SINCE_LAST', 'CODE', 'ENCOUNTER_FREQ_NEW', 'GENDER'\n]\ndata = data.drop(columns=columns_to_drop, errors='ignore')\nprint(f\"Dropped columns: {columns_to_drop}\")\n\ninpatient_codes = [2, 6, 7, 9]\nprint(f\"Unique ENCOUNTERCLASS values: {data['ENCOUNTERCLASS'].unique()}\")\nprint(f\"ENCOUNTERCLASS value counts: {data['ENCOUNTERCLASS'].value_counts().to_dict()}\")\ninpatient_count = len(data[data['ENCOUNTERCLASS'].isin(inpatient_codes)])\nprint(f\"Total patients with inpatient encounters (codes {inpatient_codes}): {inpatient_count}\")\n\ndata = data.dropna(subset=['READMISSION_30D'])\nprint(f\"Class distribution of READMISSION_30D: {data['READMISSION_30D'].value_counts().to_dict()}\")\nprint(f\"RACE unique values: {data['RACE'].unique()}\")\n\nX = data.drop('READMISSION_30D', axis=1)\ny = data['READMISSION_30D']\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n\nnumerical_cols = [\n    'BASE_ENCOUNTER_COST', 'TOTAL_CLAIM_COST', 'PAYER_COVERAGE', 'LAT', 'LON',\n    'HEALTHCARE_EXPENSES', 'HEALTHCARE_COVERAGE', 'INCOME', 'AGE'\n]\ncategorical_cols = ['ENCOUNTERCLASS', 'MARITAL', 'RACE', 'ETHNICITY']\n\nfor col in categorical_cols:\n    X_train[col] = X_train[col].astype(str)\n    X_test[col] = X_test[col].astype(str)\n    X[col] = X[col].astype(str)\n\nfor col in numerical_cols:\n    X_train[col] = pd.to_numeric(X_train[col], errors='coerce')\n    X_test[col] = pd.to_numeric(X_test[col], errors='coerce')\n    X[col] = pd.to_numeric(X[col], errors='coerce')\n\nnum_imputer = SimpleImputer(strategy='mean')\ncat_imputer = SimpleImputer(strategy='most_frequent')\n\nX_train[numerical_cols] = num_imputer.fit_transform(X_train[numerical_cols])\nX_test[numerical_cols] = num_imputer.transform(X_test[numerical_cols])\nX[numerical_cols] = num_imputer.transform(X[numerical_cols])\n\nX_train[categorical_cols] = cat_imputer.fit_transform(X_train[categorical_cols])\nX_test[categorical_cols] = cat_imputer.transform(X_test[categorical_cols])\nX[categorical_cols] = cat_imputer.transform(X[categorical_cols])\n\npreprocessor = ColumnTransformer(\n    transformers=[\n        ('num', StandardScaler(), numerical_cols),\n        ('cat', OneHotEncoder(handle_unknown='ignore', sparse_output=True), categorical_cols)\n    ]\n)\n\nX_train_transformed = preprocessor.fit_transform(X_train)\nX_test_transformed = preprocessor.transform(X_test)\nX_transformed = preprocessor.transform(X)\n\nprint(\"Applying SMOTE to address sparse RACE categories...\")\nsmote = SMOTE(random_state=42)\nX_train_transformed, y_train = smote.fit_resample(X_train_transformed, y_train)\nprint(f\"Class distribution after SMOTE: {pd.Series(y_train).value_counts().to_dict()}\")\n\nprint(\"Training and calibrating the model...\")\nbase_model = XGBClassifier(random_state=42, eval_metric='logloss')\nparam_grid = {\n    'n_estimators': [100, 200],\n    'learning_rate': [0.01, 0.05],\n    'max_depth': [3, 5],\n    'scale_pos_weight': [1]\n}\ngrid_search = GridSearchCV(base_model, param_grid, cv=3, scoring='roc_auc', n_jobs=1)\ngrid_search.fit(X_train_transformed, y_train)\nprint(f\"Best parameters: {grid_search.best_params_}\")\n\ncalibrated_model = CalibratedClassifierCV(grid_search.best_estimator_, method='sigmoid', cv=3)\ncalibrated_model.fit(X_train_transformed, y_train)\n\nscores = cross_val_score(calibrated_model, X_train_transformed, y_train, cv=5, scoring='roc_auc')\nprint(f\"Cross-Validation AUC: {scores.mean():.4f} +/- {scores.std():.4f}\")\n\nproba_all = calibrated_model.predict_proba(X_transformed)[:, 1]\n\nprint(f\"Predicted probabilities - Min: {np.min(proba_all):.4f}, Max: {np.max(proba_all):.4f}, Mean: {np.mean(proba_all):.4f}\")\n\ny_test_pred_proba = calibrated_model.predict_proba(X_test_transformed)[:, 1]\nauc = roc_auc_score(y_test, y_test_pred_proba)\nbrier = brier_score_loss(y_test, y_test_pred_proba)\nprint(f\"AUC-ROC: {auc:.4f}, Brier Score: {brier:.4f}\")\n\nprecision, recall, pr_thresholds = precision_recall_curve(y_test, y_test_pred_proba)\nif np.any((precision >= 0.95) & (recall >= 0.50)):\n    threshold = pr_thresholds[np.argmax((precision[:-1] >= 0.95) & (recall[:-1] >= 0.50))]\nelse:\n    f1_scores = 2 * (precision * recall) / (precision + recall + 1e-10)\n    threshold = pr_thresholds[np.argmax(f1_scores[:-1])]\nprint(f\"Clinically-informed threshold (precision ~0.95, recall ~0.50): {threshold:.4f}\")\n\ndata['readmission_risk'] = proba_all\ndata['high_risk_flag'] = (proba_all >= threshold).astype(int)\nprint(f\"Total patients: {len(data)}\")\nprint(f\"Number of high-risk patients (threshold={threshold}): {data['high_risk_flag'].sum()}\")\n\nthresholds = [0.098, 0.099, 0.50, 0.75, 0.90, threshold]\nfor thresh in thresholds:\n    y_test_pred = (y_test_pred_proba >= thresh).astype(int)\n    acc = accuracy_score(y_test, y_test_pred)\n    prec = precision_score(y_test, y_test_pred)\n    rec = recall_score(y_test, y_test_pred)\n    f1 = f1_score(y_test, y_test_pred)\n    print(f\"Threshold {thresh:.4f}: Accuracy = {acc:.4f}, Precision = {prec:.4f}, Recall = {rec:.4f}, F1 = {f1:.4f}\")\n\nprint(\"\\nBias Analysis:\")\nfor group in ['RACE']:\n    for value in X_test[group].unique():\n        mask = X_test[group] == value\n        y_true = y_test[mask]\n        y_pred = (y_test_pred_proba[mask] >= threshold).astype(int)\n        if len(y_true) > 0:\n            acc = accuracy_score(y_true, y_pred)\n            prec = precision_score(y_true, y_pred, zero_division=0)\n            rec = recall_score(y_true, y_pred, zero_division=0)\n            print(f\"{group} = {value}: Accuracy = {acc:.4f}, Precision = {prec:.4f}, Recall = {rec:.4f}\")\n        else:\n            print(f\"{group} = {value}: Insufficient data for analysis\")\n\nprint(\"\\nAnalyzing risk factors with SHAP...\")\nexplainer = shap.TreeExplainer(grid_search.best_estimator_)\nshap_values = explainer.shap_values(X_transformed)\nfeature_names = numerical_cols + list(preprocessor.named_transformers_['cat'].get_feature_names_out(categorical_cols))\nshap.summary_plot(shap_values, X_transformed, feature_names=feature_names, show=False)\nplt.savefig('shap_summary_plot.png')\nplt.close()\nprint(\"SHAP summary plot saved as 'shap_summary_plot.png' for portfolio.\")\n\nhigh_risk_patients = data[data['high_risk_flag'] == 1]\ninpatient_high_risk = high_risk_patients[high_risk_patients['ENCOUNTERCLASS'].isin(inpatient_codes)]\nprint(f\"High-risk patients with inpatient encounters: {len(inpatient_high_risk)}\")\nprint(f\"ENCOUNTERCLASS distribution in high-risk inpatient patients: {inpatient_high_risk['ENCOUNTERCLASS'].value_counts().to_dict()}\")\nprint(f\"Sample of high-risk inpatient encounters: {inpatient_high_risk[['ENCOUNTERCLASS', 'readmission_risk']].head().to_dict()}\")\n\ninpatient_high_risk.to_csv('high_risk_inpatients.csv', index=False)\nprint(\"Exported high-risk inpatients to 'high_risk_inpatients.csv' for Power BI visualization.\")\n\ndata.to_csv('healthcare_step4_processed.csv', index=False)\nprint(\"Processed dataset saved as 'healthcare_step4_processed.csv' for use in Step 5 and beyond.\")\n\nprint(\"\\nSpecific Intervention Recommendations:\")\nif len(inpatient_high_risk) > 0:\n    print(\"- Schedule follow-ups within 7 days for high-risk inpatients (ENCOUNTERCLASS in [2, 6, 7, 9], readmission_risk > threshold).\")\n    print(\"- Prioritize patients with low PAYER_COVERAGE (< $1000) for care coordination to address gaps.\")\n    print(\"- Assign case managers to high-risk patients with HEALTHCARE_EXPENSES > $10,000 to optimize resource allocation.\")\nelse:\n    print(\"- No high-risk patients with inpatient encounters found.\")\n    print(\"- Suggestions: Verify inpatient encounter data, ensure ENCOUNTERCLASS mapping aligns with inpatient_codes, or focus on other encounter types (e.g., outpatient).\")\n\nprint(\"\\nEnhanced Monitoring Plan:\")\nprint(\"- Track AUC-ROC, precision, recall, and F1-score monthly via automated pipelines.\")\nprint(\"- Monitor fairness metrics (e.g., recall by RACE) to detect bias drift.\")\nprint(\"- Retrain model if AUC drops below 0.80 or recall drops below 0.40, indicating performance degradation.\")\nprint(\"- Investigate concept drift due to seasonal trends (e.g., flu season) or demographic shifts using statistical tests (e.g., KS test).\")\n\nprint(\"\\nCompliance Note:\")\nprint(\"- Data storage and processing comply with HIPAA: patient identifiers removed, data encrypted with AES-256, access limited to authorized personnel via role-based access control (RBAC).\")\nprint(\"- Model card generated with details on data sources, model assumptions, performance metrics, and limitations for transparency.\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-18T12:17:33.203938Z","iopub.execute_input":"2025-05-18T12:17:33.204644Z","iopub.status.idle":"2025-05-18T12:40:35.200978Z","shell.execute_reply.started":"2025-05-18T12:17:33.204610Z","shell.execute_reply":"2025-05-18T12:40:35.199800Z"}},"outputs":[{"name":"stdout","text":"Loading dataset...\nDataset loaded with 572051 rows and 55 columns.\nCalculating READMISSION_30D based on encounter dates...\nNote: READMISSION_30D includes all encounters within 30 days; future work to refine to inpatient-only readmissions recommended.\nDropped columns: ['readmission_risk', 'high_risk_flag', 'REASONCODE', 'Id_x', 'Id_y', 'PATIENT', 'ORGANIZATION', 'PROVIDER', 'PAYER', 'START', 'STOP', 'PREV_ENCOUNTER', 'BIRTHDATE', 'DEATHDATE', 'SSN', 'DRIVERS', 'PASSPORT', 'PREFIX', 'FIRST', 'LAST', 'SUFFIX', 'MAIDEN', 'ADDRESS', 'CITY', 'STATE', 'COUNTY', 'FIPS', 'ZIP', 'BIRTHPLACE', 'DESCRIPTION', 'REASONDESCRIPTION', 'IS_INPATIENT', 'ENCOUNTER_FREQ', 'DAYS_SINCE_LAST', 'CODE', 'ENCOUNTER_FREQ_NEW', 'GENDER']\nUnique ENCOUNTERCLASS values: [5 0 1 4 6 7 2 9 8 3]\nENCOUNTERCLASS value counts: {0: 473949, 5: 54343, 1: 23382, 4: 8400, 2: 5782, 8: 2284, 3: 1341, 6: 1169, 9: 1072, 7: 329}\nTotal patients with inpatient encounters (codes [2, 6, 7, 9]): 8352\nClass distribution of READMISSION_30D: {1: 403502, 0: 168549}\nRACE unique values: ['white' 'black' 'asian' 'native' 'other' 'hawaiian']\nApplying SMOTE to address sparse RACE categories...\nClass distribution after SMOTE: {1: 322801, 0: 322801}\nTraining and calibrating the model...\nBest parameters: {'learning_rate': 0.05, 'max_depth': 5, 'n_estimators': 200, 'scale_pos_weight': 1}\nPreprocessor saved as 'preprocessor.pkl'\nModel saved as 'xgboost_model.pkl'\nCross-Validation AUC: 0.9558 +/- 0.0021\nPredicted probabilities - Min: 0.0349, Max: 0.9718, Mean: 0.6454\nAUC-ROC: 0.9528, Brier Score: 0.0813\nClinically-informed threshold (precision ~0.95, recall ~0.50): 0.3909\nTotal patients: 572051\nNumber of high-risk patients (threshold=0.39086071650187176): 384472\nThreshold 0.0980: Accuracy = 0.8719, Precision = 0.8743, Recall = 0.9559, F1 = 0.9133\nThreshold 0.0990: Accuracy = 0.8723, Precision = 0.8750, Recall = 0.9555, F1 = 0.9135\nThreshold 0.5000: Accuracy = 0.8949, Precision = 0.9570, Recall = 0.8910, F1 = 0.9228\nThreshold 0.7500: Accuracy = 0.8691, Precision = 0.9772, Recall = 0.8339, F1 = 0.8999\nThreshold 0.9000: Accuracy = 0.8267, Precision = 0.9888, Recall = 0.7629, F1 = 0.8613\nThreshold 0.3909: Accuracy = 0.8993, Precision = 0.9500, Recall = 0.9049, F1 = 0.9269\n\nBias Analysis:\nRACE = white: Accuracy = 0.8970, Precision = 0.9483, Recall = 0.9018\nRACE = asian: Accuracy = 0.9074, Precision = 0.9554, Recall = 0.9120\nRACE = black: Accuracy = 0.9049, Precision = 0.9546, Recall = 0.9142\nRACE = hawaiian: Accuracy = 0.9218, Precision = 0.9613, Recall = 0.9390\nRACE = other: Accuracy = 0.9199, Precision = 0.9624, Recall = 0.9299\nRACE = native: Accuracy = 0.9231, Precision = 0.9733, Recall = 0.9125\n\nAnalyzing risk factors with SHAP...\nSHAP summary plot saved as 'shap_summary_plot.png' for portfolio.\nHigh-risk patients with inpatient encounters: 5801\nENCOUNTERCLASS distribution in high-risk inpatient patients: {2: 5472, 7: 329}\nSample of high-risk inpatient encounters: {'ENCOUNTERCLASS': {84: 7, 85: 2, 86: 2, 87: 2, 88: 2}, 'readmission_risk': {84: 0.9437795082728068, 85: 0.949964165687561, 86: 0.9698004722595215, 87: 0.9704246520996094, 88: 0.9698004722595215}}\nExported high-risk inpatients to 'high_risk_inpatients.csv' for Power BI visualization.\nProcessed dataset saved as 'healthcare_step4_processed.csv' for use in Step 5 and beyond.\n\nSpecific Intervention Recommendations:\n- Schedule follow-ups within 7 days for high-risk inpatients (ENCOUNTERCLASS in [2, 6, 7, 9], readmission_risk > threshold).\n- Prioritize patients with low PAYER_COVERAGE (< $1000) for care coordination to address gaps.\n- Assign case managers to high-risk patients with HEALTHCARE_EXPENSES > $10,000 to optimize resource allocation.\n\nEnhanced Monitoring Plan:\n- Track AUC-ROC, precision, recall, and F1-score monthly via automated pipelines.\n- Monitor fairness metrics (e.g., recall by RACE) to detect bias drift.\n- Retrain model if AUC drops below 0.80 or recall drops below 0.40, indicating performance degradation.\n- Investigate concept drift due to seasonal trends (e.g., flu season) or demographic shifts using statistical tests (e.g., KS test).\n\nCompliance Note:\n- Data storage and processing comply with HIPAA: patient identifiers removed, data encrypted with AES-256, access limited to authorized personnel via role-based access control (RBAC).\n- Model card generated with details on data sources, model assumptions, performance metrics, and limitations for transparency.\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"pip install dash","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"pip install dash","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-18T14:36:57.493844Z","iopub.execute_input":"2025-05-18T14:36:57.494076Z","iopub.status.idle":"2025-05-18T14:37:01.167619Z","shell.execute_reply.started":"2025-05-18T14:36:57.494052Z","shell.execute_reply":"2025-05-18T14:37:01.166545Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: dash in /usr/local/lib/python3.11/dist-packages (3.0.4)\nRequirement already satisfied: Flask<3.1,>=1.0.4 in /usr/local/lib/python3.11/dist-packages (from dash) (3.0.3)\nRequirement already satisfied: Werkzeug<3.1 in /usr/local/lib/python3.11/dist-packages (from dash) (3.0.6)\nRequirement already satisfied: plotly>=5.0.0 in /usr/local/lib/python3.11/dist-packages (from dash) (5.24.1)\nRequirement already satisfied: importlib-metadata in /usr/local/lib/python3.11/dist-packages (from dash) (8.6.1)\nRequirement already satisfied: typing-extensions>=4.1.1 in /usr/local/lib/python3.11/dist-packages (from dash) (4.13.1)\nRequirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from dash) (2.32.3)\nRequirement already satisfied: retrying in /usr/local/lib/python3.11/dist-packages (from dash) (1.3.4)\nRequirement already satisfied: nest-asyncio in /usr/local/lib/python3.11/dist-packages (from dash) (1.6.0)\nRequirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from dash) (75.1.0)\nRequirement already satisfied: Jinja2>=3.1.2 in /usr/local/lib/python3.11/dist-packages (from Flask<3.1,>=1.0.4->dash) (3.1.6)\nRequirement already satisfied: itsdangerous>=2.1.2 in /usr/local/lib/python3.11/dist-packages (from Flask<3.1,>=1.0.4->dash) (2.2.0)\nRequirement already satisfied: click>=8.1.3 in /usr/local/lib/python3.11/dist-packages (from Flask<3.1,>=1.0.4->dash) (8.1.8)\nRequirement already satisfied: blinker>=1.6.2 in /usr/local/lib/python3.11/dist-packages (from Flask<3.1,>=1.0.4->dash) (1.9.0)\nRequirement already satisfied: tenacity>=6.2.0 in /usr/local/lib/python3.11/dist-packages (from plotly>=5.0.0->dash) (9.0.0)\nRequirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from plotly>=5.0.0->dash) (24.2)\nRequirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.11/dist-packages (from Werkzeug<3.1->dash) (3.0.2)\nRequirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.11/dist-packages (from importlib-metadata->dash) (3.21.0)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->dash) (3.4.1)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->dash) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->dash) (2.3.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->dash) (2025.1.31)\nRequirement already satisfied: six>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from retrying->dash) (1.17.0)\nNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"# Import required libraries\nimport dash\nfrom dash import dcc, html, dash_table\nfrom dash.dependencies import Input, Output, State\nimport plotly.express as px\nimport plotly.graph_objects as go\nimport pandas as pd\nimport nest_asyncio\nimport numpy as np\nimport sqlite3\nfrom datetime import datetime, timedelta\nimport schedule\nimport time\nimport logging\nimport requests\n\n# Enable async handling for local environments\nnest_asyncio.apply()\n\n# Set up logging\nlogging.basicConfig(filename='update_log.txt', level=logging.INFO, \n                    format='%(asctime)s - %(levelname)s - %(message)s')\n\n# Load the initial processed dataset\nprint(\"Loading initial processed dataset...\")\ndata_with_risk = pd.read_csv('/kaggle/input/healthcare-step4-processed1/healthcare_step4_processed.csv', low_memory=False)\n\n# Prepare data: Ensure necessary columns are numeric\nif 'readmission_risk' in data_with_risk.columns:\n    data_with_risk['readmission_risk'] = pd.to_numeric(data_with_risk['readmission_risk'], errors='coerce')\nelse:\n    print(\"ERROR: 'readmission_risk' column not found. Using default values.\")\n    data_with_risk['readmission_risk'] = 0.0\nif 'AGE' in data_with_risk.columns:\n    data_with_risk['AGE'] = pd.to_numeric(data_with_risk['AGE'], errors='coerce')\nelse:\n    print(\"ERROR: 'AGE' column not found.\")\n\n# Map ENCOUNTERCLASS to meaningful labels if numeric\nif 'ENCOUNTERCLASS' in data_with_risk.columns and data_with_risk['ENCOUNTERCLASS'].dtype in [np.int64, np.int32, np.float64]:\n    encounter_class_map = {\n        0: 'Outpatient', 1: 'Inpatient', 2: 'Emergency', 3: 'Observation',\n        4: 'Telehealth', 5: 'Ambulatory', 6: 'Home', 7: 'Virtual',\n        8: 'Urgent Care', 9: 'Other'\n    }\n    data_with_risk['ENCOUNTERCLASS'] = data_with_risk['ENCOUNTERCLASS'].map(encounter_class_map).fillna(data_with_risk['ENCOUNTERCLASS'].astype(str))\nprint(\"Unique ENCOUNTERCLASS values:\", data_with_risk['ENCOUNTERCLASS'].unique().tolist())\n\n# Create AGE_GROUP\ndata_with_risk['AGE_GROUP'] = pd.cut(data_with_risk['AGE'], bins=[0, 20, 40, 60, 80, 120], labels=['0-20', '21-40', '41-60', '61-80', '81+'], right=False)\ndata_with_risk['AGE_GROUP'] = data_with_risk['AGE_GROUP'].astype(str).fillna('81+')\n\n# Ensure DATE column exists\nif 'DATE' not in data_with_risk.columns:\n    if all(col in data_with_risk.columns for col in ['YEAR', 'MONTH', 'DAY']):\n        data_with_risk['DATE'] = pd.to_datetime(data_with_risk[['YEAR', 'MONTH', 'DAY']])\n    else:\n        data_with_risk['DATE'] = pd.to_datetime('2025-01-01')  # Default date\n\n# Save to in-memory SQLite database\nconn = sqlite3.connect(':memory:', check_same_thread=False)\ndata_with_risk.to_sql('patients', conn, index=False, if_exists='replace')\ncursor = conn.cursor()\ncursor.execute('CREATE INDEX idx_age_group ON patients (AGE_GROUP)')\ncursor.execute('CREATE INDEX idx_encounter_class ON patients (ENCOUNTERCLASS)')\ncursor.execute('CREATE INDEX idx_readmission_risk ON patients (readmission_risk)')\nconn.commit()\n\n# Function to fetch new patient data from Flask API\ndef fetch_new_patients():\n    global data_with_risk, conn\n    try:\n        response = requests.get('http://localhost:5000/new_patients')\n        response.raise_for_status()\n        new_patients_json = response.json()\n        new_patients = pd.DataFrame(new_patients_json)\n\n        # Ensure required columns\n        required_cols = ['AGE', 'ENCOUNTERCLASS', 'readmission_risk', 'DATE']\n        for col in required_cols:\n            if col not in new_patients.columns:\n                raise ValueError(f\"API response missing required column: {col}\")\n\n        # Process the data\n        new_patients['DATE'] = pd.to_datetime(new_patients['DATE'])\n        new_patients['AGE'] = pd.to_numeric(new_patients['AGE'], errors='coerce')\n        new_patients['readmission_risk'] = pd.to_numeric(new_patients['readmission_risk'], errors='coerce')\n        new_patients['AGE_GROUP'] = pd.cut(new_patients['AGE'], bins=[0, 20, 40, 60, 80, 120], \n                                           labels=['0-20', '21-40', '41-60', '61-80', '81+'], right=False)\n        new_patients['AGE_GROUP'] = new_patients['AGE_GROUP'].astype(str).fillna('81+')\n\n        # Append to existing data\n        data_with_risk = pd.concat([data_with_risk, new_patients], ignore_index=True)\n\n        # Update SQLite database\n        data_with_risk.to_sql('patients', conn, index=False, if_exists='replace')\n        cursor.execute('CREATE INDEX idx_age_group ON patients (AGE_GROUP)')\n        cursor.execute('CREATE INDEX idx_encounter_class ON patients (ENCOUNTERCLASS)')\n        cursor.execute('CREATE INDEX idx_readmission_risk ON patients (readmission_risk)')\n        conn.commit()\n\n        logging.info(f\"Added {len(new_patients)} new patients from API. Total rows: {len(data_with_risk)}\")\n        print(f\"Fetched new patients at {datetime.now().strftime('%H:%M:%S')}. Total rows: {len(data_with_risk)}\")\n    except Exception as e:\n        logging.error(f\"Error fetching new patients: {e}\")\n        print(f\"Error fetching new patients: {e}\")\n\n# Schedule API calls every 10 seconds\nschedule.every(10).seconds.do(fetch_new_patients)\n\n# Initialize the Dash app\napp = dash.Dash(__name__)\n\n# Define the layout\napp.layout = html.Div([\n    html.H1(\"Patient Readmission Risk Dashboard\", style={'textAlign': 'center', 'color': '#0066cc'}),\n    html.Div([\n        html.P(\"Note: High-risk patients are highlighted in red (Readmission Risk ≥ 0.3909).\", style={'textAlign': 'center', 'color': 'gray'}),\n    ], style={'marginBottom': '10px'}),\n    html.Div([\n        html.Div([\n            html.Label(\"Filter by Age Group:\"),\n            dcc.Dropdown(\n                id='age-group-filter',\n                options=[{'label': 'All', 'value': 'All'}] + [{'label': str(age_group), 'value': str(age_group)} for age_group in data_with_risk['AGE_GROUP'].unique() if pd.notna(age_group)],\n                value='All',\n                style={'width': '100%'}\n            ),\n        ], style={'width': '30%', 'display': 'inline-block', 'padding': '10px'}),\n        html.Div([\n            html.Label(\"Filter by Encounter Class:\"),\n            dcc.Dropdown(\n                id='encounter-class-filter',\n                options=[{'label': 'All', 'value': 'All'}] + [{'label': str(encounter), 'value': encounter} for encounter in data_with_risk['ENCOUNTERCLASS'].unique()],\n                value='All',\n                style={'width': '100%'}\n            ),\n        ], style={'width': '30%', 'display': 'inline-block', 'padding': '10px'}),\n        html.Div([\n            html.Label(\"Total High-Risk Patients:\"),\n            html.Div(id='kpi-card', style={'fontSize': '24px', 'textAlign': 'center'}),\n        ], style={'width': '30%', 'display': 'inline-block', 'padding': '10px'}),\n    ], style={'display': 'flex', 'justifyContent': 'space-around'}),\n    html.Div([\n        html.Div([dcc.Graph(id='scatter-plot')], style={'width': '33%', 'display': 'inline-block', 'padding': '10px'}),\n        html.Div([dcc.Graph(id='bar-chart')], style={'width': '33%', 'display': 'inline-block', 'padding': '10px'}),\n        html.Div([dcc.Graph(id='pie-chart')], style={'width': '33%', 'display': 'inline-block', 'padding': '10px'}),\n    ], style={'display': 'flex', 'justifyContent': 'space-around'}),\n    html.Div([\n        html.Div([dcc.Graph(id='trend-line')], style={'width': '50%', 'display': 'inline-block', 'padding': '10px'}),\n        html.Div([dcc.Graph(id='heatmap')], style={'width': '50%', 'display': 'inline-block', 'padding': '10px'}),\n    ], style={'display': 'flex', 'justifyContent': 'space-around'}),\n    html.Div([\n        html.H3(\"High-Risk Patients\", style={'textAlign': 'center'}),\n        html.Div(id='patient-table', style={'margin': '20px'}),\n        html.Button(\"Download High-Risk Patients as CSV\", id='download-button', style={'display': 'block', 'margin': 'auto'}),\n        dcc.Download(id='download-dataframe-csv')\n    ]),\n])\n\n# Callback to update KPI card\n@app.callback(\n    Output('kpi-card', 'children'),\n    [Input('age-group-filter', 'value'),\n     Input('encounter-class-filter', 'value')]\n)\ndef update_kpi_card(age_group, encounter_class):\n    query = \"SELECT COUNT(*) as count FROM patients WHERE readmission_risk >= 0.3909\"\n    params = []\n    if age_group != 'All':\n        query += \" AND AGE_GROUP = ?\"\n        params.append(age_group)\n    if encounter_class != 'All':\n        query += \" AND ENCOUNTERCLASS = ?\"\n        params.append(encounter_class)\n    df = pd.read_sql(query, conn, params=params)\n    return str(df['count'].iloc[0])\n\n# Callback to update scatter plot\n@app.callback(\n    Output('scatter-plot', 'figure'),\n    [Input('age-group-filter', 'value'),\n     Input('encounter-class-filter', 'value')]\n)\ndef update_scatter_plot(age_group, encounter_class):\n    query = \"SELECT AGE, ENCOUNTERCLASS, readmission_risk FROM patients\"\n    params = []\n    conditions = []\n    if age_group != 'All':\n        conditions.append(\"AGE_GROUP = ?\")\n        params.append(age_group)\n    if encounter_class != 'All':\n        conditions.append(\"ENCOUNTERCLASS = ?\")\n        params.append(encounter_class)\n    if conditions:\n        query += \" WHERE \" + \" AND \".join(conditions)\n    df = pd.read_sql(query, conn, params=params)\n    df['Risk_Level'] = np.where(df['readmission_risk'] >= 0.3909, 'High Risk', 'Low Risk')\n    return px.scatter(df, x='AGE', y='readmission_risk', color='Risk_Level', \n                      color_discrete_map={'High Risk': 'red', 'Low Risk': 'blue'}, \n                      title='Readmission Risk vs. Age', \n                      labels={'AGE': 'Patient Age', 'readmission_risk': 'Readmission Risk Score'},\n                      hover_data=['ENCOUNTERCLASS'])\n\n# Callback to update bar chart\n@app.callback(\n    Output('bar-chart', 'figure'),\n    [Input('age-group-filter', 'value'),\n     Input('encounter-class-filter', 'value')]\n)\ndef update_bar_chart(age_group, encounter_class):\n    query = \"SELECT ENCOUNTERCLASS, readmission_risk FROM patients\"\n    params = []\n    conditions = []\n    if age_group != 'All':\n        conditions.append(\"AGE_GROUP = ?\")\n        params.append(age_group)\n    if encounter_class != 'All':\n        conditions.append(\"ENCOUNTERCLASS = ?\")\n        params.append(encounter_class)\n    if conditions:\n        query += \" WHERE \" + \" AND \".join(conditions)\n    df = pd.read_sql(query, conn, params=params)\n    avg_risk_by_encounter = df.groupby('ENCOUNTERCLASS')['readmission_risk'].mean().reset_index()\n    avg_risk_by_encounter['Color'] = np.where(avg_risk_by_encounter['readmission_risk'] >= 0.3909, 'red', 'blue')\n    fig = go.Figure(data=[go.Bar(x=avg_risk_by_encounter['ENCOUNTERCLASS'], y=avg_risk_by_encounter['readmission_risk'],\n                                 marker_color=avg_risk_by_encounter['Color'], text=avg_risk_by_encounter['ENCOUNTERCLASS'],\n                                 textposition='auto')])\n    fig.update_layout(title='Average Readmission Risk by Encounter Class',\n                      xaxis_title='Encounter Class', yaxis_title='Average Risk Score', showlegend=False)\n    return fig\n\n# Callback to update pie chart\n@app.callback(\n    Output('pie-chart', 'figure'),\n    [Input('age-group-filter', 'value'),\n     Input('encounter-class-filter', 'value')]\n)\ndef update_pie_chart(age_group, encounter_class):\n    query = \"SELECT AGE_GROUP FROM patients WHERE readmission_risk >= 0.3909\"\n    params = []\n    conditions = []\n    if age_group != 'All':\n        conditions.append(\"AGE_GROUP = ?\")\n        params.append(age_group)\n    if encounter_class != 'All':\n        conditions.append(\"ENCOUNTERCLASS = ?\")\n        params.append(encounter_class)\n    if conditions:\n        query += \" AND \" + \" AND \".join(conditions)\n    df = pd.read_sql(query, conn, params=params)\n    age_group_distribution = df['AGE_GROUP'].value_counts().reset_index()\n    age_group_distribution.columns = ['AGE_GROUP', 'Count']\n    fig = px.pie(age_group_distribution, names='AGE_GROUP', values='Count',\n                 title='Distribution of High-Risk Patients by Age Group', color='AGE_GROUP',\n                 color_discrete_map={'0-20': '#FF6F61', '21-40': '#6B5B95', '41-60': '#88B04B',\n                                     '61-80': '#F7CAC9', '81+': '#92A8D1'})\n    fig.update_traces(textinfo='percent+label', pull=[0.1, 0, 0, 0, 0],\n                      hovertemplate='%{label}: %{value} patients (%{percent})<extra></extra>')\n    return fig\n\n# Callback to update trend line\n@app.callback(\n    Output('trend-line', 'figure'),\n    [Input('age-group-filter', 'value'),\n     Input('encounter-class-filter', 'value')]\n)\ndef update_trend_line(age_group, encounter_class):\n    query = \"SELECT DATE, readmission_risk FROM patients\"\n    params = []\n    conditions = []\n    if age_group != 'All':\n        conditions.append(\"AGE_GROUP = ?\")\n        params.append(age_group)\n    if encounter_class != 'All':\n        conditions.append(\"ENCOUNTERCLASS = ?\")\n        params.append(encounter_class)\n    if conditions:\n        query += \" WHERE \" + \" AND \".join(conditions)\n    df = pd.read_sql(query, conn, params=params)\n    df['DATE'] = pd.to_datetime(df['DATE'], errors='coerce')\n    df = df.dropna(subset=['DATE'])\n    if not df.empty:\n        trend_data = df.groupby(df['DATE'].dt.to_period('D').dt.to_timestamp())['readmission_risk'].mean().reset_index()\n        fig = px.line(trend_data, x='DATE', y='readmission_risk',\n                      title='Average Readmission Risk Over Time',\n                      labels={'DATE': 'Date', 'readmission_risk': 'Average Risk Score'})\n        fig.add_hline(y=0.3909, line_dash=\"dash\", line_color=\"red\", annotation_text=\"High Risk Threshold (0.3909)\",\n                      annotation_position=\"top right\")\n    else:\n        fig = go.Figure()\n        fig.update_layout(title='Average Readmission Risk Over Time', xaxis_title='Date', yaxis_title='Average Risk Score')\n    return fig\n\n# Callback to update heatmap\n@app.callback(\n    Output('heatmap', 'figure'),\n    [Input('age-group-filter', 'value'),\n     Input('encounter-class-filter', 'value')]\n)\ndef update_heatmap(age_group, encounter_class):\n    query = \"SELECT AGE_GROUP, ENCOUNTERCLASS, readmission_risk FROM patients\"\n    params = []\n    conditions = []\n    if age_group != 'All':\n        conditions.append(\"AGE_GROUP = ?\")\n        params.append(age_group)\n    if encounter_class != 'All':\n        conditions.append(\"ENCOUNTERCLASS = ?\")\n        params.append(encounter_class)\n    if conditions:\n        query += \" WHERE \" + \" AND \".join(conditions)\n    df = pd.read_sql(query, conn, params=params)\n    df = df.dropna(subset=['AGE_GROUP', 'ENCOUNTERCLASS'])\n    if not df.empty:\n        heatmap_data = df.groupby(['AGE_GROUP', 'ENCOUNTERCLASS'])['readmission_risk'].mean().reset_index()\n        heatmap_data = heatmap_data.pivot(index='AGE_GROUP', columns='ENCOUNTERCLASS', values='readmission_risk')\n        return px.imshow(heatmap_data, title='Readmission Risk by Age Group and Encounter Class',\n                         labels={'color': 'Average Risk Score'}, color_continuous_scale='Reds')\n    else:\n        return go.Figure().update_layout(title='Readmission Risk by Age Group and Encounter Class')\n\n# Callback to update the patient table\n@app.callback(\n    Output('patient-table', 'children'),\n    [Input('age-group-filter', 'value'),\n     Input('encounter-class-filter', 'value'),\n     Input('bar-chart', 'clickData'),\n     Input('pie-chart', 'clickData')]\n)\ndef update_table(age_group, encounter_class, bar_click, pie_click):\n    query = \"SELECT AGE, ENCOUNTERCLASS, AGE_GROUP, readmission_risk FROM patients WHERE readmission_risk >= 0.3909\"\n    params = []\n    conditions = []\n    if age_group != 'All':\n        conditions.append(\"AGE_GROUP = ?\")\n        params.append(age_group)\n    if encounter_class != 'All':\n        conditions.append(\"ENCOUNTERCLASS = ?\")\n        params.append(encounter_class)\n    if conditions:\n        query += \" AND \" + \" AND \".join(conditions)\n    df = pd.read_sql(query, conn, params=params)\n    if bar_click:\n        selected_encounter = bar_click['points'][0]['x']\n        df = df[df['ENCOUNTERCLASS'] == selected_encounter]\n    if pie_click:\n        selected_age_group = pie_click['points'][0]['label']\n        df = df[df['AGE_GROUP'] == selected_age_group]\n    if df.empty:\n        return dash_table.DataTable(data=[], columns=[{'name': 'No data', 'id': 'nodata'}],\n                                    style_table={'overflowX': 'auto'}, style_cell={'textAlign': 'left', 'padding': '5px'},\n                                    style_header={'backgroundColor': 'lightgrey', 'fontWeight': 'bold'})\n    df['Recommendation'] = np.where(df['readmission_risk'] > 0.5, 'Schedule Follow-Up', 'Monitor Closely')\n    table_df = df[['AGE', 'ENCOUNTERCLASS', 'AGE_GROUP', 'readmission_risk', 'Recommendation']].head(10)\n    style_data_conditional = [\n        {'if': {'filter_query': '{readmission_risk} >= 0.3909'}, 'backgroundColor': 'rgba(255, 0, 0, 0.2)', 'color': 'black'}\n    ]\n    return dash_table.DataTable(data=table_df.to_dict('records'),\n                                columns=[{'name': col, 'id': col} for col in table_df.columns],\n                                style_table={'overflowX': 'auto'}, style_cell={'textAlign': 'left', 'padding': '5px'},\n                                style_header={'backgroundColor': 'lightgrey', 'fontWeight': 'bold'},\n                                style_data_conditional=style_data_conditional)\n\n# Callback to handle CSV download\n@app.callback(\n    Output('download-dataframe-csv', 'data'),\n    [Input('download-button', 'n_clicks'),\n     Input('age-group-filter', 'value'),\n     Input('encounter-class-filter', 'value'),\n     Input('bar-chart', 'clickData'),\n     Input('pie-chart', 'clickData')],\n    prevent_initial_call=True\n)\ndef download_csv(n_clicks, age_group, encounter_class, bar_click, pie_click):\n    query = \"SELECT AGE, ENCOUNTERCLASS, AGE_GROUP, readmission_risk FROM patients WHERE readmission_risk >= 0.3909\"\n    params = []\n    conditions = []\n    if age_group != 'All':\n        conditions.append(\"AGE_GROUP = ?\")\n        params.append(age_group)\n    if encounter_class != 'All':\n        conditions.append(\"ENCOUNTERCLASS = ?\")\n        params.append(encounter_class)\n    if conditions:\n        query += \" AND \" + \" AND \".join(conditions)\n    df = pd.read_sql(query, conn, params=params)\n    if bar_click:\n        selected_encounter = bar_click['points'][0]['x']\n        df = df[df['ENCOUNTERCLASS'] == selected_encounter]\n    if pie_click:\n        selected_age_group = pie_click['points'][0]['label']\n        df = df[df['AGE_GROUP'] == selected_age_group]\n    if df.empty:\n        return dcc.send_data_frame(pd.DataFrame(columns=['No data']).to_csv, \"high_risk_patients.csv\")\n    df['Recommendation'] = np.where(df['readmission_risk'] > 0.5, 'Schedule Follow-Up', 'Monitor Closely')\n    table_df = df[['AGE', 'ENCOUNTERCLASS', 'AGE_GROUP', 'readmission_risk', 'Recommendation']]\n    return dcc.send_data_frame(table_df.to_csv, \"high_risk_patients.csv\")\n\n# Run the scheduler and app\nif __name__ == '__main__':\n    print(\"Starting scheduler and Dash app on localhost:8050...\")\n    def run_scheduler():\n        while True:\n            schedule.run_pending()\n            time.sleep(1)\n\n    import threading\n    scheduler_thread = threading.Thread(target=run_scheduler, daemon=True)\n    scheduler_thread.start()\n\n    app.run(port=8050, host='localhost', debug=True, use_reloader=False, threaded=False)\n    print(\"Dash app should be accessible at http://localhost:8050\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-18T15:38:21.377452Z","iopub.execute_input":"2025-05-18T15:38:21.377900Z","iopub.status.idle":"2025-05-18T15:38:32.160462Z","shell.execute_reply.started":"2025-05-18T15:38:21.377864Z","shell.execute_reply":"2025-05-18T15:38:32.159114Z"}},"outputs":[{"name":"stdout","text":"Loading initial processed dataset...\nUnique ENCOUNTERCLASS values: ['Ambulatory', 'Outpatient', 'Inpatient', 'Telehealth', 'Home', 'Virtual', 'Emergency', 'Other', 'Urgent Care', 'Observation']\nStarting scheduler and Dash app on localhost:8050...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.lib.display.IFrame at 0x7d96d645f810>","text/html":"\n        <iframe\n            width=\"100%\"\n            height=\"650\"\n            src=\"http://localhost:8050/\"\n            frameborder=\"0\"\n            allowfullscreen\n            \n        ></iframe>\n        "},"metadata":{}},{"name":"stdout","text":"Dash app should be accessible at http://localhost:8050\nError fetching new patients: HTTPConnectionPool(host='localhost', port=5000): Max retries exceeded with url: /new_patients (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7d96d623c990>: Failed to establish a new connection: [Errno 111] Connection refused'))\nError fetching new patients: HTTPConnectionPool(host='localhost', port=5000): Max retries exceeded with url: /new_patients (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7d96e5ac6350>: Failed to establish a new connection: [Errno 111] Connection refused'))\nError fetching new patients: HTTPConnectionPool(host='localhost', port=5000): Max retries exceeded with url: /new_patients (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7d96e5050250>: Failed to establish a new connection: [Errno 111] Connection refused'))\nError fetching new patients: HTTPConnectionPool(host='localhost', port=5000): Max retries exceeded with url: /new_patients (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7d96e5c90490>: Failed to establish a new connection: [Errno 111] Connection refused'))\nError fetching new patients: HTTPConnectionPool(host='localhost', port=5000): Max retries exceeded with url: /new_patients (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7d96e4d3e010>: Failed to establish a new connection: [Errno 111] Connection refused'))\nError fetching new patients: HTTPConnectionPool(host='localhost', port=5000): Max retries exceeded with url: /new_patients (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7d96e4d3efd0>: Failed to establish a new connection: [Errno 111] Connection refused'))\nError fetching new patients: HTTPConnectionPool(host='localhost', port=5000): Max retries exceeded with url: /new_patients (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7d96d6229f10>: Failed to establish a new connection: [Errno 111] Connection refused'))\nError fetching new patients: HTTPConnectionPool(host='localhost', port=5000): Max retries exceeded with url: /new_patients (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7d96e50509d0>: Failed to establish a new connection: [Errno 111] Connection refused'))\nError fetching new patients: HTTPConnectionPool(host='localhost', port=5000): Max retries exceeded with url: /new_patients (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7d96e5ca5f50>: Failed to establish a new connection: [Errno 111] Connection refused'))\nError fetching new patients: HTTPConnectionPool(host='localhost', port=5000): Max retries exceeded with url: /new_patients (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7d96e4c25f50>: Failed to establish a new connection: [Errno 111] Connection refused'))\nError fetching new patients: HTTPConnectionPool(host='localhost', port=5000): Max retries exceeded with url: /new_patients (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7d96e4d3e6d0>: Failed to establish a new connection: [Errno 111] Connection refused'))\nError fetching new patients: HTTPConnectionPool(host='localhost', port=5000): Max retries exceeded with url: /new_patients (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7d96d657b190>: Failed to establish a new connection: [Errno 111] Connection refused'))\nError fetching new patients: HTTPConnectionPool(host='localhost', port=5000): Max retries exceeded with url: /new_patients (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7d96e5ca7c10>: Failed to establish a new connection: [Errno 111] Connection refused'))\nError fetching new patients: HTTPConnectionPool(host='localhost', port=5000): Max retries exceeded with url: /new_patients (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7d96e5a4ab90>: Failed to establish a new connection: [Errno 111] Connection refused'))\nError fetching new patients: HTTPConnectionPool(host='localhost', port=5000): Max retries exceeded with url: /new_patients (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7d96e5ac4a90>: Failed to establish a new connection: [Errno 111] Connection refused'))\nError fetching new patients: HTTPConnectionPool(host='localhost', port=5000): Max retries exceeded with url: /new_patients (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7d96e5b03150>: Failed to establish a new connection: [Errno 111] Connection refused'))\nError fetching new patients: HTTPConnectionPool(host='localhost', port=5000): Max retries exceeded with url: /new_patients (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7d96e4d3f010>: Failed to establish a new connection: [Errno 111] Connection refused'))\nError fetching new patients: HTTPConnectionPool(host='localhost', port=5000): Max retries exceeded with url: /new_patients (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7d96e4c41310>: Failed to establish a new connection: [Errno 111] Connection refused'))\nError fetching new patients: HTTPConnectionPool(host='localhost', port=5000): Max retries exceeded with url: /new_patients (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7d96e5b01e10>: Failed to establish a new connection: [Errno 111] Connection refused'))\nError fetching new patients: HTTPConnectionPool(host='localhost', port=5000): Max retries exceeded with url: /new_patients (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7d96e4de5690>: Failed to establish a new connection: [Errno 111] Connection refused'))\nError fetching new patients: HTTPConnectionPool(host='localhost', port=5000): Max retries exceeded with url: /new_patients (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7d96e5051f90>: Failed to establish a new connection: [Errno 111] Connection refused'))\nError fetching new patients: HTTPConnectionPool(host='localhost', port=5000): Max retries exceeded with url: /new_patients (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7d96e5a4bb10>: Failed to establish a new connection: [Errno 111] Connection refused'))\nError fetching new patients: HTTPConnectionPool(host='localhost', port=5000): Max retries exceeded with url: /new_patients (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7d96d657bb50>: Failed to establish a new connection: [Errno 111] Connection refused'))\nError fetching new patients: HTTPConnectionPool(host='localhost', port=5000): Max retries exceeded with url: /new_patients (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7d96e4c5a350>: Failed to establish a new connection: [Errno 111] Connection refused'))\nError fetching new patients: HTTPConnectionPool(host='localhost', port=5000): Max retries exceeded with url: /new_patients (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7d96e50517d0>: Failed to establish a new connection: [Errno 111] Connection refused'))\nError fetching new patients: HTTPConnectionPool(host='localhost', port=5000): Max retries exceeded with url: /new_patients (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7d96e5ac6350>: Failed to establish a new connection: [Errno 111] Connection refused'))\nError fetching new patients: HTTPConnectionPool(host='localhost', port=5000): Max retries exceeded with url: /new_patients (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7d96d62edb10>: Failed to establish a new connection: [Errno 111] Connection refused'))\nError fetching new patients: HTTPConnectionPool(host='localhost', port=5000): Max retries exceeded with url: /new_patients (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7d96e4de7810>: Failed to establish a new connection: [Errno 111] Connection refused'))\nError fetching new patients: HTTPConnectionPool(host='localhost', port=5000): Max retries exceeded with url: /new_patients (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7d96e4bf61d0>: Failed to establish a new connection: [Errno 111] Connection refused'))\nError fetching new patients: HTTPConnectionPool(host='localhost', port=5000): Max retries exceeded with url: /new_patients (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7d96e5d33490>: Failed to establish a new connection: [Errno 111] Connection refused'))\nError fetching new patients: HTTPConnectionPool(host='localhost', port=5000): Max retries exceeded with url: /new_patients (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7d96e5ca3490>: Failed to establish a new connection: [Errno 111] Connection refused'))\nError fetching new patients: HTTPConnectionPool(host='localhost', port=5000): Max retries exceeded with url: /new_patients (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7d96e4316950>: Failed to establish a new connection: [Errno 111] Connection refused'))\nError fetching new patients: HTTPConnectionPool(host='localhost', port=5000): Max retries exceeded with url: /new_patients (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7d96e5b02750>: Failed to establish a new connection: [Errno 111] Connection refused'))\nError fetching new patients: HTTPConnectionPool(host='localhost', port=5000): Max retries exceeded with url: /new_patients (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7d96e5ca3350>: Failed to establish a new connection: [Errno 111] Connection refused'))\nError fetching new patients: HTTPConnectionPool(host='localhost', port=5000): Max retries exceeded with url: /new_patients (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7d96e4bf45d0>: Failed to establish a new connection: [Errno 111] Connection refused'))\nError fetching new patients: HTTPConnectionPool(host='localhost', port=5000): Max retries exceeded with url: /new_patients (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7d96e5b7a590>: Failed to establish a new connection: [Errno 111] Connection refused'))\nError fetching new patients: HTTPConnectionPool(host='localhost', port=5000): Max retries exceeded with url: /new_patients (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7d96e5a4b450>: Failed to establish a new connection: [Errno 111] Connection refused'))\nError fetching new patients: HTTPConnectionPool(host='localhost', port=5000): Max retries exceeded with url: /new_patients (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7d96e4de7050>: Failed to establish a new connection: [Errno 111] Connection refused'))\nError fetching new patients: HTTPConnectionPool(host='localhost', port=5000): Max retries exceeded with url: /new_patients (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7d96e4de5850>: Failed to establish a new connection: [Errno 111] Connection refused'))\nError fetching new patients: HTTPConnectionPool(host='localhost', port=5000): Max retries exceeded with url: /new_patients (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7d96e4d3ca10>: Failed to establish a new connection: [Errno 111] Connection refused'))\nError fetching new patients: HTTPConnectionPool(host='localhost', port=5000): Max retries exceeded with url: /new_patients (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7d96e4bf7110>: Failed to establish a new connection: [Errno 111] Connection refused'))\nError fetching new patients: HTTPConnectionPool(host='localhost', port=5000): Max retries exceeded with url: /new_patients (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7d96e4d93b90>: Failed to establish a new connection: [Errno 111] Connection refused'))\nError fetching new patients: HTTPConnectionPool(host='localhost', port=5000): Max retries exceeded with url: /new_patients (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7d96e4d3e9d0>: Failed to establish a new connection: [Errno 111] Connection refused'))\nError fetching new patients: HTTPConnectionPool(host='localhost', port=5000): Max retries exceeded with url: /new_patients (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7d96e4d3c750>: Failed to establish a new connection: [Errno 111] Connection refused'))\nError fetching new patients: HTTPConnectionPool(host='localhost', port=5000): Max retries exceeded with url: /new_patients (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7d96d623ce90>: Failed to establish a new connection: [Errno 111] Connection refused'))\nError fetching new patients: HTTPConnectionPool(host='localhost', port=5000): Max retries exceeded with url: /new_patients (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7d96e4be7ed0>: Failed to establish a new connection: [Errno 111] Connection refused'))\nError fetching new patients: HTTPConnectionPool(host='localhost', port=5000): Max retries exceeded with url: /new_patients (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7d96d62ee990>: Failed to establish a new connection: [Errno 111] Connection refused'))\nError fetching new patients: HTTPConnectionPool(host='localhost', port=5000): Max retries exceeded with url: /new_patients (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7d96d6229590>: Failed to establish a new connection: [Errno 111] Connection refused'))\nError fetching new patients: HTTPConnectionPool(host='localhost', port=5000): Max retries exceeded with url: /new_patients (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7d96e5ac65d0>: Failed to establish a new connection: [Errno 111] Connection refused'))\nError fetching new patients: HTTPConnectionPool(host='localhost', port=5000): Max retries exceeded with url: /new_patients (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7d96e4de5550>: Failed to establish a new connection: [Errno 111] Connection refused'))\nError fetching new patients: HTTPConnectionPool(host='localhost', port=5000): Max retries exceeded with url: /new_patients (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7d96e4d3f110>: Failed to establish a new connection: [Errno 111] Connection refused'))\nError fetching new patients: HTTPConnectionPool(host='localhost', port=5000): Max retries exceeded with url: /new_patients (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7d96e4bf4ed0>: Failed to establish a new connection: [Errno 111] Connection refused'))\nError fetching new patients: HTTPConnectionPool(host='localhost', port=5000): Max retries exceeded with url: /new_patients (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7d96e4bf7390>: Failed to establish a new connection: [Errno 111] Connection refused'))\nError fetching new patients: HTTPConnectionPool(host='localhost', port=5000): Max retries exceeded with url: /new_patients (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7d96d646c250>: Failed to establish a new connection: [Errno 111] Connection refused'))\nError fetching new patients: HTTPConnectionPool(host='localhost', port=5000): Max retries exceeded with url: /new_patients (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7d96e4c41350>: Failed to establish a new connection: [Errno 111] Connection refused'))\nError fetching new patients: HTTPConnectionPool(host='localhost', port=5000): Max retries exceeded with url: /new_patients (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7d96e4d3e450>: Failed to establish a new connection: [Errno 111] Connection refused'))\nError fetching new patients: HTTPConnectionPool(host='localhost', port=5000): Max retries exceeded with url: /new_patients (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7d96e5ca7110>: Failed to establish a new connection: [Errno 111] Connection refused'))\nError fetching new patients: HTTPConnectionPool(host='localhost', port=5000): Max retries exceeded with url: /new_patients (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7d96e4de6d10>: Failed to establish a new connection: [Errno 111] Connection refused'))\nError fetching new patients: HTTPConnectionPool(host='localhost', port=5000): Max retries exceeded with url: /new_patients (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7d96e4bf62d0>: Failed to establish a new connection: [Errno 111] Connection refused'))\nError fetching new patients: HTTPConnectionPool(host='localhost', port=5000): Max retries exceeded with url: /new_patients (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7d96e5a4ab90>: Failed to establish a new connection: [Errno 111] Connection refused'))\nError fetching new patients: HTTPConnectionPool(host='localhost', port=5000): Max retries exceeded with url: /new_patients (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7d96e4de7550>: Failed to establish a new connection: [Errno 111] Connection refused'))\nError fetching new patients: HTTPConnectionPool(host='localhost', port=5000): Max retries exceeded with url: /new_patients (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7d96e4d3c250>: Failed to establish a new connection: [Errno 111] Connection refused'))\nError fetching new patients: HTTPConnectionPool(host='localhost', port=5000): Max retries exceeded with url: /new_patients (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7d96e5050f50>: Failed to establish a new connection: [Errno 111] Connection refused'))\nError fetching new patients: HTTPConnectionPool(host='localhost', port=5000): Max retries exceeded with url: /new_patients (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7d96e4be7350>: Failed to establish a new connection: [Errno 111] Connection refused'))\nError fetching new patients: HTTPConnectionPool(host='localhost', port=5000): Max retries exceeded with url: /new_patients (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7d96e4bf62d0>: Failed to establish a new connection: [Errno 111] Connection refused'))\nError fetching new patients: HTTPConnectionPool(host='localhost', port=5000): Max retries exceeded with url: /new_patients (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7d96d6675750>: Failed to establish a new connection: [Errno 111] Connection refused'))\nError fetching new patients: HTTPConnectionPool(host='localhost', port=5000): Max retries exceeded with url: /new_patients (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7d96e4d3ce10>: Failed to establish a new connection: [Errno 111] Connection refused'))\nError fetching new patients: HTTPConnectionPool(host='localhost', port=5000): Max retries exceeded with url: /new_patients (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7d96e4de5410>: Failed to establish a new connection: [Errno 111] Connection refused'))\nError fetching new patients: HTTPConnectionPool(host='localhost', port=5000): Max retries exceeded with url: /new_patients (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7d96d62ec850>: Failed to establish a new connection: [Errno 111] Connection refused'))\nError fetching new patients: HTTPConnectionPool(host='localhost', port=5000): Max retries exceeded with url: /new_patients (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7d96e4bf45d0>: Failed to establish a new connection: [Errno 111] Connection refused'))\nError fetching new patients: HTTPConnectionPool(host='localhost', port=5000): Max retries exceeded with url: /new_patients (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7d96e4bc4310>: Failed to establish a new connection: [Errno 111] Connection refused'))\nError fetching new patients: HTTPConnectionPool(host='localhost', port=5000): Max retries exceeded with url: /new_patients (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7d96e4c509d0>: Failed to establish a new connection: [Errno 111] Connection refused'))\nError fetching new patients: HTTPConnectionPool(host='localhost', port=5000): Max retries exceeded with url: /new_patients (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7d96e4de5f10>: Failed to establish a new connection: [Errno 111] Connection refused'))\nError fetching new patients: HTTPConnectionPool(host='localhost', port=5000): Max retries exceeded with url: /new_patients (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7d96e5050410>: Failed to establish a new connection: [Errno 111] Connection refused'))\nError fetching new patients: HTTPConnectionPool(host='localhost', port=5000): Max retries exceeded with url: /new_patients (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7d96e5050f90>: Failed to establish a new connection: [Errno 111] Connection refused'))\nError fetching new patients: HTTPConnectionPool(host='localhost', port=5000): Max retries exceeded with url: /new_patients (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7d96e4bf6a90>: Failed to establish a new connection: [Errno 111] Connection refused'))\nError fetching new patients: HTTPConnectionPool(host='localhost', port=5000): Max retries exceeded with url: /new_patients (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7d96e4bc4490>: Failed to establish a new connection: [Errno 111] Connection refused'))\nError fetching new patients: HTTPConnectionPool(host='localhost', port=5000): Max retries exceeded with url: /new_patients (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7d96e4d3ce90>: Failed to establish a new connection: [Errno 111] Connection refused'))\nError fetching new patients: HTTPConnectionPool(host='localhost', port=5000): Max retries exceeded with url: /new_patients (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7d96e4be7650>: Failed to establish a new connection: [Errno 111] Connection refused'))\nError fetching new patients: HTTPConnectionPool(host='localhost', port=5000): Max retries exceeded with url: /new_patients (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7d96d62ed8d0>: Failed to establish a new connection: [Errno 111] Connection refused'))\nError fetching new patients: HTTPConnectionPool(host='localhost', port=5000): Max retries exceeded with url: /new_patients (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7d96d62edd90>: Failed to establish a new connection: [Errno 111] Connection refused'))\nError fetching new patients: HTTPConnectionPool(host='localhost', port=5000): Max retries exceeded with url: /new_patients (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7d96e4bc7cd0>: Failed to establish a new connection: [Errno 111] Connection refused'))\nError fetching new patients: HTTPConnectionPool(host='localhost', port=5000): Max retries exceeded with url: /new_patients (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7d96e5ac6e10>: Failed to establish a new connection: [Errno 111] Connection refused'))\nError fetching new patients: HTTPConnectionPool(host='localhost', port=5000): Max retries exceeded with url: /new_patients (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7d96d62ee450>: Failed to establish a new connection: [Errno 111] Connection refused'))\nError fetching new patients: HTTPConnectionPool(host='localhost', port=5000): Max retries exceeded with url: /new_patients (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7d96e4be6110>: Failed to establish a new connection: [Errno 111] Connection refused'))\nError fetching new patients: HTTPConnectionPool(host='localhost', port=5000): Max retries exceeded with url: /new_patients (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7d96e4be5c90>: Failed to establish a new connection: [Errno 111] Connection refused'))\nError fetching new patients: HTTPConnectionPool(host='localhost', port=5000): Max retries exceeded with url: /new_patients (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7d96e4bc7e90>: Failed to establish a new connection: [Errno 111] Connection refused'))\nError fetching new patients: HTTPConnectionPool(host='localhost', port=5000): Max retries exceeded with url: /new_patients (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7d96e4d3efd0>: Failed to establish a new connection: [Errno 111] Connection refused'))\nError fetching new patients: HTTPConnectionPool(host='localhost', port=5000): Max retries exceeded with url: /new_patients (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7d96e5052350>: Failed to establish a new connection: [Errno 111] Connection refused'))\nError fetching new patients: HTTPConnectionPool(host='localhost', port=5000): Max retries exceeded with url: /new_patients (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7d96e4bf5e90>: Failed to establish a new connection: [Errno 111] Connection refused'))\nError fetching new patients: HTTPConnectionPool(host='localhost', port=5000): Max retries exceeded with url: /new_patients (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7d96e4be5250>: Failed to establish a new connection: [Errno 111] Connection refused'))\nError fetching new patients: HTTPConnectionPool(host='localhost', port=5000): Max retries exceeded with url: /new_patients (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7d96e5d2d290>: Failed to establish a new connection: [Errno 111] Connection refused'))\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"pip install schedule dash","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-18T15:35:37.120281Z","iopub.execute_input":"2025-05-18T15:35:37.120616Z","iopub.status.idle":"2025-05-18T15:35:44.773475Z","shell.execute_reply.started":"2025-05-18T15:35:37.120571Z","shell.execute_reply":"2025-05-18T15:35:44.772260Z"}},"outputs":[{"name":"stdout","text":"Collecting schedule\n  Downloading schedule-1.2.2-py3-none-any.whl.metadata (3.8 kB)\nCollecting dash\n  Downloading dash-3.0.4-py3-none-any.whl.metadata (10 kB)\nCollecting Flask<3.1,>=1.0.4 (from dash)\n  Downloading flask-3.0.3-py3-none-any.whl.metadata (3.2 kB)\nCollecting Werkzeug<3.1 (from dash)\n  Downloading werkzeug-3.0.6-py3-none-any.whl.metadata (3.7 kB)\nRequirement already satisfied: plotly>=5.0.0 in /usr/local/lib/python3.11/dist-packages (from dash) (5.24.1)\nRequirement already satisfied: importlib-metadata in /usr/local/lib/python3.11/dist-packages (from dash) (8.6.1)\nRequirement already satisfied: typing-extensions>=4.1.1 in /usr/local/lib/python3.11/dist-packages (from dash) (4.13.1)\nRequirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from dash) (2.32.3)\nCollecting retrying (from dash)\n  Downloading retrying-1.3.4-py3-none-any.whl.metadata (6.9 kB)\nRequirement already satisfied: nest-asyncio in /usr/local/lib/python3.11/dist-packages (from dash) (1.6.0)\nRequirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from dash) (75.1.0)\nRequirement already satisfied: Jinja2>=3.1.2 in /usr/local/lib/python3.11/dist-packages (from Flask<3.1,>=1.0.4->dash) (3.1.6)\nRequirement already satisfied: itsdangerous>=2.1.2 in /usr/local/lib/python3.11/dist-packages (from Flask<3.1,>=1.0.4->dash) (2.2.0)\nRequirement already satisfied: click>=8.1.3 in /usr/local/lib/python3.11/dist-packages (from Flask<3.1,>=1.0.4->dash) (8.1.8)\nRequirement already satisfied: blinker>=1.6.2 in /usr/local/lib/python3.11/dist-packages (from Flask<3.1,>=1.0.4->dash) (1.9.0)\nRequirement already satisfied: tenacity>=6.2.0 in /usr/local/lib/python3.11/dist-packages (from plotly>=5.0.0->dash) (9.0.0)\nRequirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from plotly>=5.0.0->dash) (24.2)\nRequirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.11/dist-packages (from Werkzeug<3.1->dash) (3.0.2)\nRequirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.11/dist-packages (from importlib-metadata->dash) (3.21.0)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->dash) (3.4.1)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->dash) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->dash) (2.3.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->dash) (2025.1.31)\nRequirement already satisfied: six>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from retrying->dash) (1.17.0)\nDownloading schedule-1.2.2-py3-none-any.whl (12 kB)\nDownloading dash-3.0.4-py3-none-any.whl (7.9 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.9/7.9 MB\u001b[0m \u001b[31m51.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading flask-3.0.3-py3-none-any.whl (101 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m101.7/101.7 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading werkzeug-3.0.6-py3-none-any.whl (227 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m228.0/228.0 kB\u001b[0m \u001b[31m11.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading retrying-1.3.4-py3-none-any.whl (11 kB)\nInstalling collected packages: Werkzeug, schedule, retrying, Flask, dash\n  Attempting uninstall: Werkzeug\n    Found existing installation: Werkzeug 3.1.3\n    Uninstalling Werkzeug-3.1.3:\n      Successfully uninstalled Werkzeug-3.1.3\n  Attempting uninstall: Flask\n    Found existing installation: Flask 3.1.0\n    Uninstalling Flask-3.1.0:\n      Successfully uninstalled Flask-3.1.0\nSuccessfully installed Flask-3.0.3 Werkzeug-3.0.6 dash-3.0.4 retrying-1.3.4 schedule-1.2.2\nNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"# Install the correct Scikit-learn version for compatibility\n!pip install scikit-learn==1.5.0\n\nfrom flask import Flask, jsonify\nimport pandas as pd\nimport numpy as np\nfrom datetime import datetime\nimport joblib\nimport os\nimport time\nimport threading\n\napp = Flask(__name__)\n\n# Verify dataset paths\nprint(\"Checking available datasets in /kaggle/input/...\")\nprint(os.listdir('/kaggle/input/'))\n\n# Load the raw dataset\nprint(\"Loading raw dataset...\")\nstart_time = time.time()\ndata_path = \"/kaggle/input/healthcare-data-csv/healthcare_data.csv\"\nif not os.path.exists(data_path):\n    raise FileNotFoundError(f\"Raw dataset at '{data_path}' not found. Please check the path.\")\ndata = pd.read_csv(data_path, low_memory=False)  # Address DtypeWarning\nprint(f\"Dataset loading took {time.time() - start_time:.2f} seconds.\")\nprint(f\"Dataset loaded with {len(data)} rows and {len(data.columns)} columns.\")\n\n# Reduce dataset size for testing\nprint(\"Reducing dataset size for testing...\")\ndata = data.head(10000)  # Use only the first 10,000 rows\nprint(f\"Reduced dataset to {len(data)} rows.\")\n\n# Preprocess the data\nprint(\"Starting preprocessing...\")\nstart_time = time.time()\ntry:\n    data['START'] = pd.to_datetime(data['START'], errors='coerce')\n    birth_dates = pd.to_datetime(data['BIRTHDATE'], errors='coerce')\n    time_diff = (datetime.now() - birth_dates).dt.total_seconds() / (365.25 * 24 * 60 * 60)\n    data['AGE'] = time_diff.astype(float).round(0).astype(int)\nexcept KeyError as e:\n    print(f\"Error processing data: {e}. Check column names.\")\n    data['AGE'] = 0\n\nencounter_class_map = {\n    'ambulatory': 'Ambulatory', 'outpatient': 'Outpatient', 'inpatient': 'Inpatient',\n    'telehealth': 'Telehealth', 'home': 'Home', 'virtual': 'Virtual',\n    'emergency': 'Emergency', 'other': 'Other', 'urgentcare': 'Urgent Care',\n    'observation': 'Observation'\n}\ndata['ENCOUNTERCLASS'] = data['ENCOUNTERCLASS'].map(encounter_class_map).fillna('Other')\nprint(f\"Preprocessing took {time.time() - start_time:.2f} seconds.\")\n\n# Verify model dataset paths\nprint(\"Checking model dataset in /kaggle/input/...\")\nmodel_dataset = [d for d in os.listdir('/kaggle/input/') if 'xgboost_model' in d.lower()]\nif not model_dataset:\n    raise FileNotFoundError(\"Model dataset not found.\")\nmodel_dataset_path = f\"/kaggle/input/{model_dataset[0]}\"\nprint(f\"Model dataset found: {os.listdir(model_dataset_path)}\")\n\n# Dynamically determine the correct subpath\nsubpath = 'scikitlearn'\nif os.path.exists(f\"{model_dataset_path}/{subpath}\"):\n    subpath_files = os.listdir(f\"{model_dataset_path}/{subpath}\")\n    if 'default' in subpath_files:\n        subpath += '/default'\n    if '1' in os.listdir(f\"{model_dataset_path}/{subpath}\"):\n        subpath += '/1'\n    prep_path = f\"{model_dataset_path}/{subpath}/preprocessor.pkl\"\n    model_path = f\"{model_dataset_path}/{subpath}/xgboost_model.pkl\"\nelse:\n    prep_path = f\"{model_dataset_path}/preprocessor.pkl\"\n    model_path = f\"{model_dataset_path}/xgboost_model.pkl\"\n\nif not os.path.exists(prep_path):\n    raise FileNotFoundError(f\"Preprocessor file at '{prep_path}' not found.\")\nif not os.path.exists(model_path):\n    raise FileNotFoundError(f\"Model file at '{model_path}' not found.\")\npreprocessor = joblib.load(prep_path)\nmodel = joblib.load(model_path)\nprint(\"Preprocessor and model loaded successfully.\")\n\n# Define columns used in training\nnumerical_cols = [\n    'BASE_ENCOUNTER_COST', 'TOTAL_CLAIM_COST', 'PAYER_COVERAGE', 'LAT', 'LON',\n    'HEALTHCARE_EXPENSES', 'HEALTHCARE_COVERAGE', 'INCOME', 'AGE'\n]\ncategorical_cols = ['ENCOUNTERCLASS', 'MARITAL', 'RACE', 'ETHNICITY']\n\ntotal_patients = len(data)\ncurrent_index = 0\n\n@app.route('/new_patients', methods=['GET'])\ndef get_new_patients():\n    global current_index\n    batch_size = 10\n    end_index = min(current_index + batch_size, total_patients)\n    if end_index <= current_index:\n        return jsonify({\"error\": \"No more patients to serve\"}), 400\n    batch = data.iloc[current_index:end_index].copy()\n\n    try:\n        for col in numerical_cols + categorical_cols:\n            if col not in batch.columns:\n                batch[col] = np.nan\n        for col in categorical_cols:\n            batch[col] = batch[col].astype(str)\n        for col in numerical_cols:\n            batch[col] = pd.to_numeric(batch[col], errors='coerce')\n        batch[numerical_cols] = batch[numerical_cols].fillna(batch[numerical_cols].mean())\n        for col in categorical_cols:\n            batch[col] = batch[col].fillna(batch[col].mode()[0])\n        batch_transformed = preprocessor.transform(batch)\n        risk_scores = model.predict_proba(batch_transformed)[:, 1]\n        batch['readmission_risk'] = risk_scores\n        batch['DATE'] = datetime.now().isoformat()\n        new_patients = batch[['AGE', 'ENCOUNTERCLASS', 'readmission_risk', 'DATE']].to_dict(orient='records')\n    except Exception as e:\n        return jsonify({\"error\": f\"Prediction failed: {e}\"}), 500\n\n    current_index = end_index\n    if current_index >= total_patients:\n        current_index = 0\n    return jsonify(new_patients)\n\nif __name__ == '__main__':\n    print(\"Starting Flask app on port 5001 without reloader...\")\n    def run_flask():\n        try:\n            print(\"Binding to port 5001...\")\n            app.run(host='0.0.0.0', port=5001, debug=False, use_reloader=False)\n            print(\"Flask app is running on http://0.0.0.0:5001\")\n        except Exception as e:\n            print(f\"Failed to start Flask app: {e}\")\n\n    # Run Flask in a background thread\n    flask_thread = threading.Thread(target=run_flask, daemon=True)\n    flask_thread.start()\n    print(\"Flask app running in background. Use http://127.0.0.1:5001/new_patients to test.\")\n    # Keep the notebook interactive\n    time.sleep(5)  # Wait briefly to ensure Flask starts\n    print(\"Testing Flask server...\")\n    try:\n        import requests\n        response = requests.get('http://127.0.0.1:5001/new_patients', timeout=10)\n        print(\"Flask server responded successfully:\")\n        print(response.json())\n    except Exception as e:\n        print(f\"Failed to connect to Flask server: {e}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-19T09:55:29.252111Z","iopub.execute_input":"2025-05-19T09:55:29.252511Z","iopub.status.idle":"2025-05-19T09:56:00.829249Z","shell.execute_reply.started":"2025-05-19T09:55:29.252482Z","shell.execute_reply":"2025-05-19T09:56:00.828176Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: scikit-learn==1.5.0 in /usr/local/lib/python3.11/dist-packages (1.5.0)\nRequirement already satisfied: numpy>=1.19.5 in /usr/local/lib/python3.11/dist-packages (from scikit-learn==1.5.0) (1.26.4)\nRequirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn==1.5.0) (1.15.2)\nRequirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn==1.5.0) (1.4.2)\nRequirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn==1.5.0) (3.6.0)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.19.5->scikit-learn==1.5.0) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.19.5->scikit-learn==1.5.0) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.19.5->scikit-learn==1.5.0) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.19.5->scikit-learn==1.5.0) (2025.1.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.19.5->scikit-learn==1.5.0) (2022.1.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.19.5->scikit-learn==1.5.0) (2.4.1)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.19.5->scikit-learn==1.5.0) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.19.5->scikit-learn==1.5.0) (2022.1.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.19.5->scikit-learn==1.5.0) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.19.5->scikit-learn==1.5.0) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.19.5->scikit-learn==1.5.0) (2024.2.0)\nChecking available datasets in /kaggle/input/...\n['healthcare-data-engineered-csv-gz', 'healthcare-data-timeseries-csv-gz', 'xgboost_model.pkl', 'healthcare-step4-processed1', 'healthcare-data-csv', 'healthcare-data-final-csv-gz']\nLoading raw dataset...\nDataset loading took 20.84 seconds.\nDataset loaded with 1000000 rows and 43 columns.\nReducing dataset size for testing...\nReduced dataset to 10000 rows.\nStarting preprocessing...\nPreprocessing took 0.02 seconds.\nChecking model dataset in /kaggle/input/...\nModel dataset found: ['scikitlearn']\nPreprocessor and model loaded successfully.\nStarting Flask app on port 5001 without reloader...\nBinding to port 5001...\n * Serving Flask app '__main__'\nFlask app running in background. Use http://127.0.0.1:5001/new_patients to test.\n * Debug mode: off\nTesting Flask server...\nFlask server responded successfully:\n[{'AGE': 23, 'DATE': '2025-05-19T09:56:00.819557', 'ENCOUNTERCLASS': 'Outpatient', 'readmission_risk': 0.058884064356486}, {'AGE': 23, 'DATE': '2025-05-19T09:56:00.819557', 'ENCOUNTERCLASS': 'Outpatient', 'readmission_risk': 0.058884064356486}, {'AGE': 23, 'DATE': '2025-05-19T09:56:00.819557', 'ENCOUNTERCLASS': 'Ambulatory', 'readmission_risk': 0.04281246786316236}, {'AGE': 23, 'DATE': '2025-05-19T09:56:00.819557', 'ENCOUNTERCLASS': 'Outpatient', 'readmission_risk': 0.058884064356486}, {'AGE': 23, 'DATE': '2025-05-19T09:56:00.819557', 'ENCOUNTERCLASS': 'Ambulatory', 'readmission_risk': 0.05153920501470566}, {'AGE': 23, 'DATE': '2025-05-19T09:56:00.819557', 'ENCOUNTERCLASS': 'Outpatient', 'readmission_risk': 0.058884064356486}, {'AGE': 23, 'DATE': '2025-05-19T09:56:00.819557', 'ENCOUNTERCLASS': 'Emergency', 'readmission_risk': 0.04617896551887194}, {'AGE': 23, 'DATE': '2025-05-19T09:56:00.819557', 'ENCOUNTERCLASS': 'Other', 'readmission_risk': 0.054994831482569374}, {'AGE': 65, 'DATE': '2025-05-19T09:56:00.819557', 'ENCOUNTERCLASS': 'Inpatient', 'readmission_risk': 0.09043787668148677}, {'AGE': 65, 'DATE': '2025-05-19T09:56:00.819557', 'ENCOUNTERCLASS': 'Ambulatory', 'readmission_risk': 0.13176757842302322}]\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"import requests\nresponse = requests.get('http://127.0.0.1:5001/new_patients')\nprint(response.status_code)\nprint(response.json())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-19T09:56:05.546104Z","iopub.execute_input":"2025-05-19T09:56:05.546738Z","iopub.status.idle":"2025-05-19T09:56:05.583061Z","shell.execute_reply.started":"2025-05-19T09:56:05.546702Z","shell.execute_reply":"2025-05-19T09:56:05.581947Z"}},"outputs":[{"name":"stdout","text":"200\n[{'AGE': 65, 'DATE': '2025-05-19T09:56:05.574447', 'ENCOUNTERCLASS': 'Ambulatory', 'readmission_risk': 0.09532517691453297}, {'AGE': 65, 'DATE': '2025-05-19T09:56:05.574447', 'ENCOUNTERCLASS': 'Ambulatory', 'readmission_risk': 0.09375143051147461}, {'AGE': 65, 'DATE': '2025-05-19T09:56:05.574447', 'ENCOUNTERCLASS': 'Emergency', 'readmission_risk': 0.12761741876602173}, {'AGE': 65, 'DATE': '2025-05-19T09:56:05.574447', 'ENCOUNTERCLASS': 'Ambulatory', 'readmission_risk': 0.10419641683499019}, {'AGE': 65, 'DATE': '2025-05-19T09:56:05.574447', 'ENCOUNTERCLASS': 'Ambulatory', 'readmission_risk': 0.09380381802717845}, {'AGE': 65, 'DATE': '2025-05-19T09:56:05.574447', 'ENCOUNTERCLASS': 'Ambulatory', 'readmission_risk': 0.10419641683499019}, {'AGE': 57, 'DATE': '2025-05-19T09:56:05.574447', 'ENCOUNTERCLASS': 'Ambulatory', 'readmission_risk': 0.06235965465505918}, {'AGE': 57, 'DATE': '2025-05-19T09:56:05.574447', 'ENCOUNTERCLASS': 'Ambulatory', 'readmission_risk': 0.07005156079928081}, {'AGE': 57, 'DATE': '2025-05-19T09:56:05.574447', 'ENCOUNTERCLASS': 'Ambulatory', 'readmission_risk': 0.0475310335556666}, {'AGE': 57, 'DATE': '2025-05-19T09:56:05.574447', 'ENCOUNTERCLASS': 'Ambulatory', 'readmission_risk': 0.06378511836131413}]\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"!lsof -i :5001","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-19T09:54:44.509944Z","iopub.execute_input":"2025-05-19T09:54:44.510374Z","iopub.status.idle":"2025-05-19T09:54:45.774319Z","shell.execute_reply.started":"2025-05-19T09:54:44.510337Z","shell.execute_reply":"2025-05-19T09:54:45.772859Z"}},"outputs":[{"name":"stdout","text":"COMMAND PID USER   FD   TYPE DEVICE SIZE/OFF NODE NAME\npython3  31 root   58u  IPv4  37480      0t0  TCP *:5001 (LISTEN)\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"!kill -9 31","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-19T09:55:05.449344Z","iopub.execute_input":"2025-05-19T09:55:05.449744Z","execution_failed":"2025-05-19T09:55:06.957Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}